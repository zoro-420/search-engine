[
  {
    "title": "Computer science",
    "content": "\n\n\n\nComputer science is the study of computation, information, and automation.&#91;1&#93;&#91;2&#93;&#91;3&#93; Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software).&#91;4&#93;&#91;5&#93;&#91;6&#93; \n\nAlgorithms and data structures are central to computer science.&#91;7&#93;\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\n\nThe fundamental concern of computer science is determining what can and cannot be automated.&#91;2&#93;&#91;8&#93;&#91;3&#93;&#91;9&#93;&#91;10&#93; The Turing Award is generally recognized as the highest distinction in computer science.&#91;11&#93;&#91;12&#93;\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.&#91;16&#93;\n\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623.&#91;17&#93; In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.&#91;18&#93; Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry&#91;note 1&#93; when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.&#91;19&#93; He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".&#91;20&#93; \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"&#91;20&#93; making it infinitely programmable.&#91;note 2&#93; In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.&#91;21&#93; Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published&#91;22&#93; the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,&#91;23&#93; and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.&#91;24&#93;&#91;25&#93; In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,&#91;26&#93; on which commands could be typed and the results printed automatically.&#91;27&#93; In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business&#91;28&#93; to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".&#91;29&#93;\n\n\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.&#91;30&#93; As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.&#91;31&#93; Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.&#91;32&#93; Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.&#91;33&#93;&#91;34&#93; The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.&#91;35&#93; Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\nAlthough first proposed in 1956,&#91;36&#93; the term \"computer science\" appears in a 1959 article in Communications of the ACM,&#91;37&#93;\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.&#91;38&#93; Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.&#91;37&#93;\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.&#91;39&#93; Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.&#91;40&#93; Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,&#91;41&#93; to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\n\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.&#91;42&#93; Three months later in the same journal, comptologist was suggested, followed next year by hypologist.&#91;43&#93; The term computics has also been suggested.&#91;44&#93; In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh).&#91;45&#93; \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"&#91;46&#93;\n\nA folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\"&#91;note 3&#93; The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\n\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.&#91;33&#93; Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.&#91;36&#93;\n\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined.&#91;47&#93; David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.&#91;48&#93;\n\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\n\nDespite the word science in its name, there is debate over whether or not computer science is a discipline of science,&#91;49&#93; mathematics,&#91;50&#93; or engineering.&#91;51&#93; Allen Newell and Herbert A. Simon argued in 1975, .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}\nComputer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.&#91;51&#93;\n It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.&#91;51&#93; Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.&#91;51&#93; They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.&#91;51&#93;\n\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods.&#91;51&#93; Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.&#91;51&#93;\n\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.&#91;52&#93; Peter Denning's working group argued that they are theory, abstraction (modeling), and design.&#91;33&#93; Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences,&#91;53&#93; identifiable in some branches of artificial intelligence).&#91;54&#93;\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.&#91;55&#93;\n\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.&#91;56&#93;&#91;57&#93;\nCSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)&#91;58&#93;—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.&#91;56&#93;\n\nTheoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\"&#91;3&#93; Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n\nThe famous P = NP? problem, one of the Millennium Prize Problems,&#91;59&#93; is an open problem in the theory of computation.\n\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.&#91;60&#93;\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n&#91;61&#93;\n\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\n\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.&#91;62&#93; The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.\n\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,&#91;63&#93; as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.&#91;64&#93;\n\nHuman–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.\n\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\n\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.&#91;65&#93; Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.&#91;66&#93; A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model.&#91;67&#93; When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.&#91;68&#93;\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\n\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.&#91;69&#93; Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:&#91;70&#93;\n\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.&#91;76&#93;\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.&#91;77&#93;&#91;78&#93; One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.&#91;79&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Computer_science"
  },
  {
    "title": "Rajni Ganth",
    "content": "\n\nA computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster. It is sometimes named general purpose computer to distinguish it from a computer appliance.\n\nA broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users.&#91;citation needed&#93;\n\nEarly computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries.&#91;citation needed&#93;\n\nConventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved.&#91;citation needed&#93;\n\nIt was not until the mid-20th century that the word acquired its modern definition; according to the Oxford English Dictionary, the first known use of the word computer was in a different sense, in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: \"I haue &#32;&#91;sic&#93; read the truest computer of Times, and the best Arithmetician that euer &#32;&#91;sic&#93; breathed, and he reduceth thy dayes into a short number.\" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued to have the same meaning until the middle of the 20th century. During the latter part of this period, women were often hired as computers because they could be paid less than their male counterparts.&#91;1&#93; By 1943, most human computers were women.&#91;2&#93;\n\nThe Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an \"agent noun from compute (v.)\". The Online Etymology Dictionary states that the use of the term to mean \"'calculating machine' (of any type) is from 1897.\" The Online Etymology Dictionary indicates that the \"modern use\" of the term, to mean 'programmable digital electronic computer' dates from \"1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine\".&#91;3&#93; The name has remained, although modern computers are capable of many higher-level functions.\n\nDevices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was most likely a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, likely livestock or grains, sealed in hollow unbaked clay containers.&#91;a&#93;&#91;4&#93; The use of counting rods is one example.\n\nThe abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BCE. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.&#91;5&#93;\n\nThe Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.&#91;6&#93; It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c.&#8201;100 BCE. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.&#91;7&#93;\n\nMany mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.&#91;8&#93; The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BCE and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer&#91;9&#93;&#91;10&#93; and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.&#91;11&#93; Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,&#91;12&#93; an early fixed-wired knowledge processing machine&#91;13&#93; with a gear train and gear-wheels,&#91;14&#93; c.&#8201;1000 AD.\n\nThe sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.\n\nThe planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.\n\nThe slide rule was invented around 1620–1630, by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.\n\nIn the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.&#91;15&#93;\n\nIn 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which through a system of pulleys and cylinders could predict the perpetual calendar for every year from 0 CE (that is, 1 BCE) to 4000 CE, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.\n\nThe differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.&#91;16&#93; In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.\n\nIn the 1890s, the Spanish engineer Leonardo Torres Quevedo began to develop a series of advanced analog machines that could solve real and complex roots of polynomials,&#91;17&#93;&#91;18&#93;&#91;19&#93;&#91;20&#93; which were published in 1901 by the Paris Academy of Sciences.&#91;21&#93;\n\nCharles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\",&#91;22&#93; he conceptualized and invented the first mechanical computer in the early 19th century.\n\nAfter working on his difference engine he announced his invention in 1822, in a paper to the Royal Astronomical Society, titled \"Note on the application of machinery to the computation of astronomical and mathematical tables\".&#91;23&#93; He also designed to aid in navigational calculations, in 1833 he realized that a much more general design, an analytical engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The engine would incorporate an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.&#91;24&#93;&#91;25&#93;\n\nThe machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.\n\nIn his work Essays on Automatics published in 1914, Leonardo Torres Quevedo wrote a brief history of Babbage's efforts at constructing a mechanical Difference Engine and Analytical Engine. The paper contains a design of a machine capable to calculate formulas like \n  \n    \n      \n        \n          a\n          \n            x\n          \n        \n        (\n        y\n        &#x2212;\n        z\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{x}(y-z)^{2}}\n  \n, for a sequence of sets of values. The whole machine was to be controlled by a read-only program, which was complete with provisions for conditional branching. He also introduced the idea of floating-point arithmetic.&#91;26&#93;&#91;27&#93;&#91;28&#93; In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, which allowed a user to input arithmetic problems through a keyboard, and computed and printed the results,&#91;29&#93;&#91;30&#93;&#91;31&#93;&#91;32&#93; demonstrating the feasibility of an electromechanical analytical engine.&#91;33&#93;\n\nDuring the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.&#91;34&#93; The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.&#91;16&#93;\n\nThe art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious.&#91;citation needed&#93; By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems).&#91;citation needed&#93;\n\nClaude Shannon's 1937 master's thesis laid the foundations of digital computing, with his insight of applying Boolean algebra to the analysis and synthesis of switching circuits being the basic concept which underlies all electronic digital computers.&#91;35&#93;&#91;36&#93;\n\nBy 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target.&#91;citation needed&#93; During World War II similar devices were developed in other countries as well.&#91;citation needed&#93;\n\nEarly digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939 in Berlin, was one of the earliest examples of an electromechanical relay computer.&#91;37&#93;\n\nIn 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.&#91;40&#93;&#91;41&#93; The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz.&#91;42&#93; Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating-point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.&#91;43&#93; The Z3 was not itself a universal computer but could be extended to be Turing complete.&#91;44&#93;&#91;45&#93;\n\nZuse's next computer, the Z4, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the ETH Zurich.&#91;46&#93; The computer was manufactured by Zuse's own company, Zuse KG, which was founded in 1941 as the first company with the sole purpose of developing computers in Berlin.&#91;46&#93; The Z4 served as the inspiration for the construction of the ERMETH, the first Swiss computer and one of the first in Europe.&#91;47&#93;\n\n\nPurely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.&#91;34&#93; In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,&#91;48&#93; the first \"automatic electronic digital computer\".&#91;49&#93; This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.&#91;50&#93;\n\nDuring World War II, the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women.&#91;51&#93;&#91;52&#93; To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.&#91;50&#93; He spent eleven months from early February 1943 designing and building the first Colossus.&#91;53&#93; After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944&#91;54&#93; and attacked its first message on 5 February.&#91;50&#93;\n\nColossus was the world's first electronic digital programmable computer.&#91;34&#93; It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.&#91;55&#93;&#91;56&#93;\n\nThe ENIAC&#91;57&#93; (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the \"ENIAC girls\".&#91;58&#93;&#91;59&#93;\n\nIt combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.&#91;60&#93;\n\nThe principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper,&#91;61&#93; On Computable Numbers. Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.&#91;62&#93; Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.\n\nEarly computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.&#91;50&#93; With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid out by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report \"Proposed Electronic Calculator\" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.&#91;34&#93;\n\nThe Manchester Baby was the world's first stored-program computer. It was built at the University of Manchester in England by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.&#91;63&#93; It was designed as a testbed for the Williams tube, the first random-access digital storage device.&#91;64&#93; Although the computer was described as \"small and primitive\" by a 1998 retrospective, it was the first working machine to contain all of the elements essential to a modern electronic computer.&#91;65&#93; As soon as the Baby had demonstrated the feasibility of its design, a project began at the university to develop it into a practically useful computer, the Manchester Mark 1.\n\nThe Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.&#91;66&#93; Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.&#91;67&#93; In October 1947 the directors of British catering company J. Lyons &amp; Company decided to take an active role in promoting the commercial development of computers. Lyons's LEO I computer, modelled closely on the Cambridge EDSAC of 1949, became operational in April 1951&#91;68&#93; and ran the world's first routine office computer job.\n\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947, which was followed by Shockley's bipolar junction transistor in 1948.&#91;69&#93;&#91;70&#93; From 1955 onwards, transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialized applications.&#91;71&#93;\n\nAt the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.&#91;72&#93; Their first transistorized computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125&#160;kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,&#91;73&#93; built by the electronics division of the Atomic Energy Research Establishment at Harwell.&#91;73&#93;&#91;74&#93;\n\nThe metal–oxide–silicon field-effect transistor (MOSFET), also known as the MOS transistor, was invented at Bell Labs between 1955 and 1960&#91;75&#93;&#91;76&#93;&#91;77&#93;&#91;78&#93;&#91;79&#93;&#91;80&#93; and was the first truly compact transistor that could be miniaturized and mass-produced for a wide range of uses.&#91;71&#93; With its high scalability,&#91;81&#93; and much lower power consumption and higher density than bipolar junction transistors,&#91;82&#93; the MOSFET made it possible to build high-density integrated circuits.&#91;83&#93;&#91;84&#93; In addition to data processing, it also enabled the practical use of MOS transistors as memory cell storage elements, leading to the development of MOS semiconductor memory, which replaced earlier magnetic-core memory in computers. The MOSFET led to the microcomputer revolution,&#91;85&#93; and became the driving force behind the computer revolution.&#91;86&#93;&#91;87&#93; The MOSFET is the most widely used transistor in computers,&#91;88&#93;&#91;89&#93; and is the fundamental building block of digital electronics.&#91;90&#93;\n\nThe next great advance in computing power came with the advent of the integrated circuit (IC).\nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington,&#160;D.C., on 7 May 1952.&#91;91&#93;\n\nThe first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.&#91;92&#93; Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.&#91;93&#93; In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material&#160;... wherein all the components of the electronic circuit are completely integrated\".&#91;94&#93;&#91;95&#93; However, Kilby's invention was a hybrid integrated circuit (hybrid IC), rather than a monolithic integrated circuit (IC) chip.&#91;96&#93; Kilby's IC had external wire connections, which made it difficult to mass-produce.&#91;97&#93;\n\nNoyce also came up with his own idea of an integrated circuit half a year later than Kilby.&#91;98&#93; Noyce's invention was the first true monolithic IC chip.&#91;99&#93;&#91;97&#93; His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. Noyce's monolithic IC was fabricated using the planar process, developed by his colleague Jean Hoerni in early 1959. In turn, the planar process was based on Carl Frosch and Lincoln Derick work on semiconductor surface passivation by silicon dioxide.&#91;100&#93;&#91;101&#93;&#91;102&#93;&#91;103&#93;&#91;104&#93;&#91;105&#93;\n\nModern monolithic ICs are predominantly MOS (metal–oxide–semiconductor) integrated circuits, built from MOSFETs (MOS transistors).&#91;106&#93; The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962.&#91;107&#93; General Microelectronics later introduced the first commercial MOS IC in 1964,&#91;108&#93; developed by Robert Norman.&#91;107&#93; Following the development of the self-aligned gate (silicon-gate) MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967, the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968.&#91;109&#93; The MOSFET has since become the most critical device component in modern ICs.&#91;106&#93;\n\nThe development of the MOS integrated circuit led to the invention of the microprocessor,&#91;110&#93;&#91;111&#93; and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004,&#91;112&#93; designed and realized by Federico Faggin with his silicon-gate MOS IC technology,&#91;110&#93; along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel.&#91;b&#93;&#91;114&#93; In the early 1970s, MOS IC technology enabled the integration of more than 10,000 transistors on a single chip.&#91;84&#93;\n\nSystem on a Chip (SoCs) are complete computers on a microchip (or chip) the size of a coin.&#91;115&#93; They may or may not have integrated RAM and flash memory. If not integrated, the RAM is usually placed directly above (known as Package on package) or below (on the opposite side of the circuit board) the SoC, and the flash memory is usually placed right next to the SoC. This is done to improve data transfer speeds, as the data signals do not have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power.\n\nThe first mobile computers were heavy and ran from mains power. The 50&#160;lb (23&#160;kg) IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s.&#91;116&#93; The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s.\n\nThese smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market.&#91;117&#93; These are powered by System on a Chip (SoCs), which are complete computers on a microchip the size of a coin.&#91;115&#93;\n\nComputers can be classified in a number of different ways, including:\n\nThe term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and \"mice\" input devices are all hardware.\n\nA general-purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.\n\nWhen unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n\nThe means through which computer gives output are known as output devices. Some examples of output devices are:\n\nThe control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.&#91;d&#93; Control systems in advanced computers may change the order of execution of some instructions to improve performance.\n\nA key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.&#91;e&#93;\n\nThe control system's function is as follows— this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:\n\nSince the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).\n\nThe sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.\n\nThe control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor.\n\nThe ALU is capable of performing two classes of operations: arithmetic and logic.&#91;122&#93; The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can operate only on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return Boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing Boolean logic.\n\nSuperscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.&#91;123&#93; Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.\n\nA computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.\n\nIn almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.\n\nThe CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.\n\nComputer main memory comes in two principal varieties:\n\nRAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.&#91;f&#93;\n\nIn more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.\n\nI/O is the means by which a computer exchanges information with the outside world.&#91;125&#93; Devices that provide input or output to the computer are called peripherals.&#91;126&#93; On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.\nI/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.&#91;citation needed&#93; Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.\n\nWhile a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.&#91;127&#93; One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn.&#91;128&#93;\n\nBefore the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.\n\nSome computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed in only large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.\n\nSupercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers.&#91;g&#93; They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to use most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.\n\nSoftware refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software. Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called \"firmware\".\n\nThere are thousands of different programming languages—some intended for general purpose, others useful for only highly specialized applications.\n\nThe defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.\n\nThis section applies to most common RAM machine–based computers.\n\nIn most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called \"jump\" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction.\n\nProgram execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.\n\nComparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:\n\nOnce told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.\n\nIn most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture.&#91;130&#93;&#91;131&#93; In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.\n\nWhile it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,&#91;h&#93; it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember&#160;– a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.\n\nProgramming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.\n\nMachine languages and the assembly languages that represent them (collectively termed low-level programming languages) are generally unique to the particular architecture of a computer's central processing unit (CPU). For instance, an ARM architecture CPU (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.&#91;i&#93; Historically a significant number of other CPU architectures were created and saw extensive use, notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80.\n\nAlthough considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.&#91;j&#93; High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.\n\nProgram design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable.&#91;132&#93; As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered.&#91;133&#93; Large programs involving thousands of line of code and more require formal software methodologies.&#91;134&#93; The task of developing large software systems presents a significant intellectual challenge.&#91;135&#93; Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult;&#91;136&#93; the academic and professional discipline of software engineering concentrates specifically on this challenge.&#91;137&#93;\n\nErrors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. However, in some cases they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash.&#91;138&#93; Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.&#91;k&#93; Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.&#91;139&#93;\n\nComputers have been used to coordinate information between multiple physical locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.&#91;140&#93;\n\nIn the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.&#91;141&#93; The technologies that made the Arpanet possible spread and evolved. In time, the network spread beyond academic and military institutions and became known as the Internet.\n\nThe emergence of networking involved a redefinition of the nature and boundaries of computers. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s, computer networking become almost ubiquitous, due to the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL.\n\nThe number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.\n\nA computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer,&#91;l&#93; a typical modern definition of a computer is: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\"&#91;142&#93; According to this definition, any device that processes information qualifies as a computer.\n\nThere is active research to make unconventional computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.\n\nThere are many types of computer architectures:\n\nOf all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.&#91;143&#93; Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.\n\nA computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule-based systems and pattern recognition systems. Rule-based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern-based systems use data about a problem to generate conclusions. Examples of pattern-based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.\n\nAs the use of computers has spread throughout society, there are an increasing number of careers involving computers.\n\nThe need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.\n\n\n",
    "url": "https://en.wikipedia.org/wiki/Computer"
  },
  {
    "title": "Programming language",
    "content": "\n\n\nA programming language is a system of notation for writing computer programs.&#91;1&#93;\nProgramming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables, and mechanisms for error handling. An implementation of a programming language is required in order to execute programs, namely an interpreter or a compiler. An interpreter directly executes the source code, while a compiler produces an executable program.\n\nComputer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity.\n\nThousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.\n\nProgramming languages differ from natural languages in that natural languages are used for interaction between people, while programming languages are designed to allow humans to communicate instructions to machines.&#91;citation needed&#93;\n\nThe term computer language is sometimes used interchangeably with \"programming language\".&#91;2&#93; However, usage of these terms varies among authors.\n\nIn one usage, programming languages are described as a subset of computer languages.&#91;3&#93; Similarly, the term \"computer language\" may be used in contrast to the term \"programming language\" to describe languages used in computing but not considered programming languages&#91;citation needed&#93;.  Most practical programming languages are Turing complete,&#91;4&#93; and as such are equivalent in what programs they can compute.\n\nAnother usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.&#91;5&#93; John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.&#91;6&#93;\n\nThe first programmable computers were invented at the end of the 1940s, and with them, the first programming languages.&#91;7&#93; The earliest computers were programmed in first-generation programming languages (1GLs), machine language (simple instructions that could be directly executed by the processor). This code was very difficult to debug and was not portable between different computer systems.&#91;8&#93; In order to improve the ease of programming, assembly languages (or second-generation programming languages—2GLs) were invented, diverging from the machine language to make programs easier to understand for humans, although they did not increase portability.&#91;9&#93;\n\nInitially, hardware resources were scarce and expensive, while human resources were cheaper. Therefore, cumbersome languages that were time-consuming to use, but were closer to the hardware for higher efficiency were favored.&#91;10&#93; The introduction of high-level programming languages (third-generation programming languages—3GLs)—revolutionized programming. These languages abstracted away the details of the hardware, instead being designed to express algorithms that could be understood more easily by humans. For example, arithmetic expressions could now be written in symbolic notation and later translated into machine code that the hardware could execute.&#91;9&#93; In 1957, Fortran (FORmula TRANslation) was invented. Often considered the first compiled high-level programming language,&#91;9&#93;&#91;11&#93; Fortran has remained in use into the twenty-first century.&#91;12&#93;\n\nAround 1960, the first mainframes—general purpose computers—were developed, although they could only be operated by professionals and the cost was extreme. The data and instructions were input by punch cards, meaning that no input could be added while the program was running. The languages developed at this time therefore are designed for minimal interaction.&#91;14&#93; After the invention of the microprocessor, computers in the 1970s became dramatically cheaper.&#91;15&#93; New computers also allowed more user interaction, which was supported by newer programming languages.&#91;16&#93;\n\nLisp, implemented in 1958, was the first functional programming language.&#91;17&#93; Unlike Fortran, it supported recursion and conditional expressions,&#91;18&#93; and it also introduced dynamic memory management on a heap and automatic garbage collection.&#91;19&#93; For the next decades, Lisp dominated artificial intelligence applications.&#91;20&#93; In 1978, another functional language, ML, introduced inferred types and polymorphic parameters.&#91;16&#93;&#91;21&#93;\n\nAfter ALGOL (ALGOrithmic Language) was released in 1958 and 1960,&#91;22&#93; it became the standard in computing literature for describing algorithms. Although its commercial success was limited, most popular imperative languages—including C, Pascal, Ada, C++, Java, and C#—are directly or indirectly descended from ALGOL 60.&#91;23&#93;&#91;12&#93; Among its innovations adopted by later programming languages included greater portability and the first use of context-free, BNF grammar.&#91;24&#93; Simula, the first language to support object-oriented programming (including subtypes, dynamic dispatch, and inheritance), also descends from ALGOL and achieved commercial success.&#91;25&#93; C, another ALGOL descendant, has sustained popularity into the twenty-first century. C allows access to lower-level machine operations more than other contemporary languages. Its power and efficiency, generated in part with flexible pointer operations, comes at the cost of making it more difficult to write correct code.&#91;16&#93;\n\nProlog, designed in 1972, was the first logic programming language, communicating with a computer using formal logic notation.&#91;26&#93;&#91;27&#93; With logic programming, the programmer specifies a desired result and allows the interpreter to decide how to achieve it.&#91;28&#93;&#91;27&#93;\n\nDuring the 1980s, the invention of the personal computer transformed the roles for which programming languages were used.&#91;29&#93; New languages introduced in the 1980s included C++, a superset of C that can compile C programs but also supports classes and inheritance.&#91;30&#93; Ada and other new languages introduced support for concurrency.&#91;31&#93; The Japanese government invested heavily into the so-called fifth-generation languages that added support for concurrency to logic programming constructs, but these languages were outperformed by other concurrency-supporting languages.&#91;32&#93;&#91;33&#93;\n\nDue to the rapid growth of the Internet and the World Wide Web in the 1990s, new programming languages were introduced to support Web pages and networking.&#91;34&#93; Java, based on C++ and designed for increased portability across systems and security, enjoyed large-scale success because these features are essential for many Internet applications.&#91;35&#93;&#91;36&#93; Another development was that of dynamically typed scripting languages—Python, JavaScript, PHP, and Ruby—designed to quickly produce small programs that coordinate existing applications. Due to their integration with HTML, they have also been used for building web pages hosted on servers.&#91;37&#93;&#91;38&#93;\n\nDuring the 2000s, there was a slowdown in the development of new programming languages that achieved widespread popularity.&#91;39&#93; One innovation was service-oriented programming, designed to exploit distributed systems whose components are connected by a network. Services are similar to objects in object-oriented programming, but run on a separate process.&#91;40&#93; C# and F# cross-pollinated ideas between imperative and functional programming.&#91;41&#93; After 2010, several new languages—Rust, Go, Swift, Zig and Carbon —competed for the performance-critical software for which C had historically been used.&#91;42&#93; Most of the new programming languages use static typing while a few numbers of new languages use dynamic typing like Ring and Julia.&#91;43&#93;&#91;44&#93;\n\nSome of the new programming languages are classified as visual programming languages like Scratch, LabVIEW and PWCT. Also, some of these languages mix between textual and visual programming usage like Ballerina.&#91;45&#93;&#91;46&#93;&#91;47&#93;&#91;48&#93; Also, this trend lead to developing projects that help in developing new VPLs like Blockly by Google.&#91;49&#93; Many game engines like Unreal and Unity added support for visual scripting too.&#91;50&#93;&#91;51&#93;\n\nEvery programming language includes fundamental elements for describing data and the operations or transformations applied to them, such as adding two numbers or selecting an item from a collection. These elements are governed by syntactic and semantic rules that define their structure and meaning, respectively.\n\nA programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are graphical, using visual relationships between symbols to specify a program.\n\nThe syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.\n\nThe programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:\n\nThis grammar specifies the following:\n\nThe following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).\n\nNot all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.\n\nUsing natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:\n\nThe following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p &gt;&gt; 4 has no meaning for a value having a complex type and p-&gt;im is not defined because the value of p is the null pointer):\n\nIf the type declaration on the first line were omitted, the program would trigger an error on the undefined variable p during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information.\n\nThe grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.&#91;52&#93; Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.&#91;53&#93; In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.&#91;54&#93;\n\nThe term semantics refers to the meaning of languages, as opposed to their form (syntax).\n\nStatic semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.&#91;1&#93;&#91;failed verification&#93; For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.&#91;55&#93; Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Programming languages such as Java and C# have definite assignment analysis, a form of data flow analysis, as part of their respective static semantics.&#91;56&#93;\n\nOnce data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research goes into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.&#91;56&#93;\n\nA data type is a set of allowable values and operations that can be performed on these values.&#91;57&#93; Each programming language's type system defines which data types exist, the type of an expression, and how type equivalence and type compatibility function in the language.&#91;58&#93;\n\nAccording to type theory, a language is fully typed if the specification of every operation defines types of data to which the operation is applicable.&#91;59&#93; In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, generally sequences of bits of various lengths.&#91;59&#93; In practice, while few languages are fully typed, most offer a degree of typing.&#91;59&#93; \n\nBecause different types (such as integers and floats) represent values differently, unexpected results will occur if one type is used when another is expected. Type checking will flag this error, usually at compile time (runtime type checking is more costly).&#91;60&#93; With strong typing, type errors can always be detected unless variables are explicitly cast to a different type. Weak typing occurs when languages allow implicit casting—for example, to enable operations between variables of different types without the programmer making an explicit type conversion. The more cases in which this type coercion is allowed, the fewer type errors can be detected.&#91;61&#93;  \n\nEarly programming languages often supported only built-in, numeric types such as the integer (signed and unsigned) and floating point (to support operations on real numbers that are not integers). Most programming languages support multiple sizes of floats (often called float and double) and integers depending on the size and precision required by the programmer. Storing an integer in a type that is too small to represent it leads to integer overflow. The most common way of representing negative numbers with signed types is twos complement, although ones complement is also used.&#91;62&#93; Other common types include Boolean—which is either true or false—and character—traditionally one byte, sufficient to represent all ASCII characters.&#91;63&#93; \n\nArrays are a data type whose elements, in many languages, must consist of a single type of fixed length. Other languages define arrays as references to data stored elsewhere and support elements of varying types.&#91;64&#93; Depending on the programming language, sequences of multiple characters, called strings, may be supported as arrays of characters or their own primitive type.&#91;65&#93; Strings may be of fixed or variable length, which enables greater flexibility at the cost of increased storage space and more complexity.&#91;66&#93; Other data types that may be supported include lists,&#91;67&#93; associative (unordered) arrays accessed via keys,&#91;68&#93; records in which data is mapped to names in an ordered structure,&#91;69&#93; and tuples—similar to records but without names for data fields.&#91;70&#93; Pointers store memory addresses, typically referencing locations on the heap where other data is stored.&#91;71&#93;\n\nThe simplest user-defined type is an ordinal type, often called an enumeration, whose values can be mapped onto the set of positive integers.&#91;72&#93; Since the mid-1980s, most programming languages also support abstract data types, in which the representation of the data and operations are hidden from the user, who can only access an interface.&#91;73&#93; The benefits of data abstraction can include increased reliability, reduced complexity, less potential for name collision, and allowing the underlying data structure to be changed without the client needing to alter its code.&#91;74&#93;\n\nIn static typing, all expressions have their types determined before a program executes, typically at compile-time.&#91;59&#93; Most widely used, statically typed programming languages require the types of variables to be specified explicitly. In some languages, types are implicit; one form of this is when the compiler can infer types based on context. The downside of implicit typing is the potential for errors to go undetected.&#91;75&#93; Complete type inference has traditionally been associated with functional languages such as Haskell and ML.&#91;76&#93; \n\nWith dynamic typing, the type is not attached to the variable but only the value encoded in it. A single variable can be reused for a value of a different type. Although this provides more flexibility to the programmer, it is at the cost of lower reliability and less ability for the programming language to check for errors.&#91;77&#93; Some languages allow variables of a union type to which any type of value can be assigned, in an exception to their usual static typing rules.&#91;78&#93;\n\nIn computing, multiple instructions can be executed simultaneously. Many programming languages support instruction-level and subprogram-level concurrency.&#91;79&#93; By the twenty-first century, additional processing power on computers was increasingly coming from the use of additional processors, which requires programmers to design software that makes use of multiple processors simultaneously to achieve improved performance.&#91;80&#93; Interpreted languages such as Python and Ruby do not support the concurrent use of multiple processors.&#91;81&#93; Other programming languages do support managing data shared between different threads by controlling the order of execution of key instructions via the use of semaphores, controlling access to shared data via monitor, or enabling message passing between threads.&#91;82&#93;\n\nMany programming languages include exception handlers, a section of code triggered by runtime errors that can deal with them in two main ways:&#91;83&#93;\n\nSome programming languages support dedicating a block of code to run regardless of whether an exception occurs before the code is reached; this is called finalization.&#91;84&#93;\n\nThere is a tradeoff between increased ability to handle exceptions and reduced performance.&#91;85&#93; For example, even though array index errors are common&#91;86&#93; C does not check them for performance reasons.&#91;85&#93; Although programmers can write code to catch user-defined exceptions, this can clutter a program. Standard libraries in some languages, such as C, use their return values to indicate an exception.&#91;87&#93; Some languages and their compilers have the option of turning on and off error handling capability, either temporarily or permanently.&#91;88&#93;\n\nOne of the most important influences on programming language design has been computer architecture. Imperative languages, the most commonly used type, were designed to perform well on von Neumann architecture, the most common computer architecture.&#91;89&#93; In von Neumann architecture, the memory stores both data and instructions, while the CPU that performs instructions on data is separate, and data must be piped back and forth to the CPU. The central elements in these languages are variables, assignment, and iteration, which is more efficient than recursion on these machines.&#91;90&#93;  \n\nMany programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse.&#91;citation needed&#93; The birth of programming languages in the 1950s was stimulated by the desire to make a universal programming language suitable for all machines and uses, avoiding the need to write code for different computers.&#91;91&#93; By the early 1960s, the idea of a universal language was rejected due to the differing requirements of the variety of purposes for which code was written.&#91;92&#93; \n\nDesirable qualities of programming languages include readability, writability, and reliability.&#91;93&#93; These features can reduce the cost of training programmers in a language, the amount of time needed to write and maintain programs in the language, the cost of compiling the code, and increase runtime performance.&#91;94&#93; \n\nProgramming language design often involves tradeoffs.&#91;104&#93; For example, features to improve reliability typically come at the cost of performance.&#91;105&#93; Increased expressivity due to a large number of operators makes writing code easier but comes at the cost of readability.&#91;105&#93;\n\n\nNatural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs.&#91;106&#93; Alan Perlis was similarly dismissive of the idea.&#91;107&#93; \n\nThe specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.\n\nA programming language specification can take several forms, including the following:\n\nAn implementation of a programming language is the conversion of a program into machine code that can be executed by the hardware. The machine code then can be executed with the help of the operating system.&#91;111&#93; The most common form of interpretation in production code is by a compiler, which translates the source code via an intermediate-level language into machine code, known as an executable. Once the program is compiled, it will run more quickly than with other implementation methods.&#91;112&#93; Some compilers are able to provide further optimization to reduce memory or computation usage when the executable runs, but increasing compilation time.&#91;113&#93;\n\nAnother implementation method is to run the program with an interpreter, which translates each line of software into machine code just before it executes. Although it can make debugging easier, the downside of interpretation is that it runs 10 to 100 times slower than a compiled executable.&#91;114&#93; Hybrid interpretation methods provide some of the benefits of compilation and some of the benefits of interpretation via partial compilation. One form this takes is just-in-time compilation, in which the software is compiled ahead of time into an intermediate language, and then into machine code immediately before execution.&#91;115&#93;\n\nAlthough most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.&#91;citation needed&#93;\n\nSome programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,&#91;116&#93; and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.&#91;117&#93;\n\nMany proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.&#91;118&#93;\n\nOpen source programming languages are particularly helpful for open science applications, enhancing the capacity for replication and code sharing.&#91;119&#93;\n\nThousands of different programming languages have been created, mainly in the computing field.&#91;120&#93;\nIndividual software projects commonly use five programming languages or more.&#91;121&#93;\n\nProgramming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers \"do exactly what they are told to do\", and cannot \"understand\" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.\n\nA programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).&#91;122&#93; Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.\n\nPrograms for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the \"commands\" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.&#91;123&#93;\n\nDetermining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;&#91;124&#93;&#91;125&#93; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.\n\nVarious methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:\n\nCombining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.&#91;129&#93;\n\nAs of June 2024, the top five programming languages as measured by TIOBE index are Python, C++, C, Java and C#. TIOBE provide a list of top 100 programming languages according to popularity and update this list every month.&#91;130&#93;\n\nA dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.\n\nProgramming languages are often placed into four main categories: imperative, functional, logic, and object oriented.&#91;131&#93;\n\nAlthough markup languages are not programming languages, some have extensions that support limited programming. Additionally, there are special-purpose languages that are not easily compared to other programming languages.&#91;135&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Programming_language"
  },
  {
    "title": "Software engineering",
    "content": "Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.&#91;1&#93;&#91;2&#93;&#91;3&#93;&#91;4&#93;\n\nThe terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload.&#91;5&#93;\n\nA software engineer applies a software development process,&#91;1&#93;&#91;6&#93; which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.\n\nBeginning in the 1960s, software engineering was recognized as a separate field of engineering. \n\nThe development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. \n\nIn 1968, NATO held the first software engineering conference where issues related to software were addressed. Guidelines and best practices for the development of software were established.&#91;7&#93;\n\nThe origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\"&#91;8&#93; and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in \"President's Letter to the ACM Membership\" by Anthony A. Oettinger.&#91;9&#93;&#91;10&#93;&#91;11&#93; It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer.&#91;12&#93; Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy.&#91;13&#93; At the time there was perceived to be a \"software crisis\".&#91;14&#93;&#91;15&#93;&#91;16&#93; The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of Frederick Brooks&#91;17&#93; and Margaret Hamilton.&#91;18&#93;\n\nIn 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States.&#91;19&#93;\nWatts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.&#91;19&#93; The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team.\n\nModern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).&#91;6&#93; Software engineering is considered one of the major computing disciplines.&#91;20&#93;\n\nNotable definitions of software engineering include:\n\nThe term has also been used less formally:\n\nMargaret Hamilton promoted the term \"software engineering\" during her work on the Apollo program. The term \"engineering\" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:\nWhen I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new \"term\" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.&#91;30&#93;\nIndividual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering.&#91;31&#93;&#91;32&#93; Steve McConnell has said that it is not, but that it should be.&#91;33&#93; Donald Knuth has said that programming is an art and a science.&#91;34&#93; Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States.&#91;35&#93;\n\nRequirements engineering is about elicitation, analysis, specification, and validation of requirements for software. Software requirements can be functional, non-functional or domain. \n\nFunctional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.&#91;36&#93;\n\nSoftware design is the process of making high-level plans for the software. Design is sometimes divided into levels: \n\nSoftware construction typically involves programming (a.k.a. coding), unit testing, integration testing, and debugging so as to implement the design.&#91;1&#93;&#91;6&#93;“Software testing is related to, but different from, ... debugging”.&#91;6&#93;\nTesting during this phase is generally performed by the programmer and with the purpose to verify that the code behaves as designed and to know when the code is ready for the next level of testing.&#91;citation needed&#93;\n\nSoftware testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test.&#91;1&#93;&#91;6&#93;\n\nWhen described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it.  It is performed at the system level and is considered an aspect of software quality.\n\nProgram analysis is the process of analyzing computer programs with respect to an aspect such as performance, robustness, and security.\n\nSoftware maintenance refers to supporting the software after release. It may include but is not limited to: error correction, optimization, deletion of unused and discarded features, and enhancement of existing features.&#91;1&#93;&#91;6&#93; \n\nUsually, maintenance takes up 40% to 80% of project cost.&#91;38&#93;\n\nKnowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.&#91;39&#93;\nMany software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014.&#91;20&#93; A number of universities have Software Engineering degree programs; as of 2010&#91;update&#93;, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.\n\nIn addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.\n\nHalf of all practitioners today have degrees in computer science, information systems, or information technology.&#91;citation needed&#93; A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program.&#91;40&#93; In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University.&#91;41&#93; In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.&#91;citation needed&#93;\n\nSince then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004&#91;update&#93;, about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.\n\nIn 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world.&#91;citation needed&#93; Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers.&#91;42&#93; ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.&#91;6&#93;\n\nLegal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario,&#91;43&#93; and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.\n\nIn the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized.&#91;44&#93; NCEES ended the exam after April 2019 due to lack of participation.&#91;45&#93; Mandatory licensing is currently still largely debated, and perceived as controversial.&#91;46&#93;&#91;47&#93; \n\nThe IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4.&#91;6&#93; The IEEE also promulgates a \"Software Engineering Code of Ethics\".&#91;48&#93;\n\nThere are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.&#91;49&#93;&#91;50&#93;\n\nMany software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers.\n\nMost software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008.&#91;51&#93; Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis, Obesity, and hand and wrist problems such as carpal tunnel syndrome.&#91;52&#93;\n\nThe U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018.&#91;53&#93; Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.&#91;54&#93; The BLS estimates from 2023 to 2033 that computer software engineering would increase by 17%.&#91;55&#93; This is down from the 2022 to 2032 BLS estimate of 25% for software engineering.&#91;55&#93;&#91;56&#93; And, is further down from their 30% 2010 to 2020 BLS estimate.&#91;57&#93; Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries.&#91;58&#93;&#91;51&#93; In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031.&#91;58&#93; and then a decline of -11 percent from 2022 to 2032.&#91;58&#93; Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower.&#91;58&#93;&#91;59&#93;&#91;60&#93; Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields.&#91;61&#93; Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers.&#91;62&#93;&#91;63&#93;&#91;64&#93;&#91;65&#93;&#91;66&#93;&#91;67&#93;&#91;68&#93; However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades.&#91;58&#93;\n\nThe Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture.&#91;69&#93; IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies.&#91;70&#93; These certification programs are tailored to the institutions that would employ people who use these technologies.\n\nBroader certification of general software engineering skills is available through various professional societies. As of 2006&#91;update&#93;, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP).&#91;71&#93; In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).&#91;72&#93; The ACM had a professional certification program in the early 1980s,&#91;citation needed&#93; which was discontinued due to lack of interest. The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s,\nbut eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering.&#91;46&#93; John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002.&#91;47&#93;\n\nIn the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).&#91;73&#93; In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.&#91;74&#93; The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.\n\nThe initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.&#91;75&#93; Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.&#91;76&#93; Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.&#91;77&#93; When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.\n\nWhile global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations).&#91;78&#93; Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.\n\nThere are various prizes in the field of software engineering:\n\n\n\n\nSome call for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.&#91;82&#93;\n\nSome claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.&#91;83&#93;\n\nSome claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\"&#91;83&#93;\n\nEdsger Dijkstra, a founder of many of the concepts in software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what he called the \"radical novelty\" of computer science:\n\n\nA number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\"&#91;84&#93;",
    "url": "https://en.wikipedia.org/wiki/Software_engineering"
  },
  {
    "title": "Data structure",
    "content": "In computer science, a data structure is a data organization and storage format that is usually chosen for efficient access to data.&#91;1&#93;&#91;2&#93;&#91;3&#93; More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data,&#91;4&#93; i.e., it is an algebraic structure about data.\n\nData structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.&#91;5&#93;\n\nDifferent types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,&#91;6&#93; while compiler implementations usually use hash tables to look up identifiers.&#91;7&#93;\n\nData structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.&#91;8&#93;\n\nData structures can be implemented using a variety of programming languages and techniques, but they all share the common goal of efficiently organizing and storing data.&#91;9&#93; Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. This approach to data structuring has profound implications for the efficiency and scalability of algorithms. For instance, the contiguous memory allocation in arrays facilitates rapid access and modification operations, leading to optimized performance in sequential data processing scenarios.&#91;10&#93; \n\nThe implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).&#91;11&#93;\n\nThere are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:&#91;12&#93;\n\nA trie, or prefix tree, is a special type of tree used to efficiently retrieve strings. In a trie, each node represents a character of a string, and the edges between nodes represent the characters that connect them. This structure is especially useful for tasks like autocomplete, spell-checking, and creating dictionaries. Tries allow for quick searches and operations based on string prefixes.\n\nMost assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.&#91;14&#93;&#91;15&#93;\n\nMost programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.\n\nModern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.\n\nMany known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.&#91;16&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Data_structure"
  },
  {
    "title": "Algorithm",
    "content": "\n\nIn mathematics and computer science, an algorithm (/ˈælɡərɪðəm/&#32;ⓘ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation.&#91;1&#93; Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).\n\nIn contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results.&#91;2&#93; For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation.\n\nAs an effective method, an algorithm can be expressed within a finite amount of space and time&#91;3&#93; and in a well-defined formal language&#91;4&#93; for calculating a function.&#91;5&#93; Starting from an initial state and initial input (perhaps empty),&#91;6&#93; the instructions describe a computation that, when executed, proceeds through a finite&#91;7&#93; number of well-defined successive states, eventually producing \"output\"&#91;8&#93; and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.&#91;9&#93;\n\nAround 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\").&#91;1&#93; In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath.&#91;10&#93; Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\".&#91;2&#93; Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term.&#91;3&#93;&#91;4&#93;&#91;clarification needed&#93; In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.&#91;citation needed&#93;\n\nOne informal definition is \"a set of rules that precisely defines a sequence of operations\",&#91;11&#93;&#91;need quotation to verify&#93; which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure&#91;12&#93;\nor cook-book recipe.&#91;13&#93; In general, a program is an algorithm only if it stops eventually&#91;14&#93;—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey &amp; 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.&#91;15&#93;\n\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.\n\nStep-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC),&#91;16&#93; Egyptian mathematics (around 1550 BC),&#91;16&#93; Indian mathematics (around 800 BC and later),&#91;17&#93;&#91;18&#93; the Ifa Oracle (around 500 BC),&#91;19&#93; Greek mathematics (around 240 BC),&#91;20&#93; Chinese mathematics (around 200 BC and later),&#91;21&#93; and Arabic mathematics (around 800 AD).&#91;22&#93;\n\nThe earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c.&#8201;2500 BC describes the earliest division algorithm.&#91;16&#93; During the Hammurabi dynasty c.&#8201;1800&#160;– c.&#8201;1600 BC, Babylonian clay tablets described algorithms for computing formulas.&#91;23&#93; Algorithms were also used in Babylonian astronomy.&#91;citation needed&#93; Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.&#91;24&#93;\n\nAlgorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c.&#8201;1550 BC.&#91;16&#93; Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,&#91;25&#93;&#91;20&#93;&#58;&#8202;Ch 9.2&#8202; and the Euclidean algorithm, which was first described in Euclid's Elements (c.&#8201;300 BC).&#91;20&#93;&#58;&#8202;Ch 9.1&#8202;Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.&#91;17&#93;\n\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.&#91;22&#93;\n\nBolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the Middle Ages],\" specifically the verge escapement mechanism&#91;26&#93; producing the tick and tock of a mechanical clock. \"The accurate automatic machine\"&#91;27&#93; led immediately to \"mechanical automata\" in the 13th century and \"computational machines\"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century.&#91;28&#93; Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called \"history's first programmer\".\n\nBell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and \"telephone switching technologies\" led to the development of the first computers.&#91;29&#93; By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c.&#8201;1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c.&#8201;1910) with its punched-paper use of Baudot code on tape.\n\nTelephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".&#91;30&#93;&#91;31&#93;\n\nIn 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\"&#91;32&#93; or \"effective method\".&#91;33&#93; Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms.\n\nThere are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called \"sets of quadruples\", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description.&#91;34&#93; A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine.&#91;34&#93; An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states.&#91;34&#93; In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.&#91;34&#93;\n\nThe graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure.\n\nIt is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of &#8288;\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n&#8288;, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of &#8288;\n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\displaystyle O(1)}\n  \n&#8288;, otherwise &#8288;\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n&#8288; is required.\n\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost &#8288;\n  \n    \n      \n        O\n        (\n        log\n        &#x2061;\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n  \n&#8288;) outperforms a sequential search (cost &#8288;\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n&#8288; ) when used for table lookups on sorted lists or arrays.\n\nThe analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\n\nEmpirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.&#91;35&#93;\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.&#91;36&#93; In general, speed improvements depend on special properties of the problem, which are very common in practical applications.&#91;37&#93; Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nAlgorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,&#91;38&#93; with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.&#91;39&#93;\n\nPer the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\".&#91;40&#93; Tausworthe augments the three Böhm-Jacopini canonical structures:&#91;41&#93; SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.&#91;42&#93; An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.&#91;43&#93;\n\nBy themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial,&#91;44&#93; and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nAnother way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:\n\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nOne of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as:\n\nHigh-level description:\n\n(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n",
    "url": "https://en.wikipedia.org/wiki/Algorithm"
  },
  {
    "title": "Operating system",
    "content": "\n\n\nAn operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.\n\nTime-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources.\n\nFor hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,&#91;1&#93;&#91;2&#93; although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer&#160;&#8211;&#32;from cellular phones and video game consoles to web servers and supercomputers.\n\nAs of September&#160;2024&#91;update&#93;, Android is the most popular operating system with a 46% market share, followed by Microsoft Windows at 26%, iOS and iPadOS at 18%, macOS at 5%, and Linux at 1%. Android, iOS, and iPadOS are mobile operating systems, while Windows, macOS, and Linux are desktop operating systems.&#91;3&#93; Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems),&#91;4&#93;&#91;5&#93; such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.\n\nSome operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. a LiveUSB from a USB stick).\n\nAn operating system is difficult to define,&#91;6&#93; but has been called \"the layer of software that manages a computer's resources for its users and their applications\".&#91;7&#93; Operating systems include the software that is always running, called a kernel—but can include other software as well.&#91;6&#93;&#91;8&#93; The two other types of programs that can run on a computer are system programs—which are associated with the operating system, but may not be part of the kernel—and applications—all other software.&#91;8&#93;\n\nThere are three main purposes that an operating system fulfills:&#91;9&#93;\n\nWith multiprocessors multiple CPUs share memory. A multicomputer or cluster computer has multiple CPUs, each of which has its own memory. Multicomputers were developed because large multiprocessors are difficult to engineer and prohibitively expensive;&#91;17&#93; they are universal in cloud computing because of the size of the machine needed.&#91;18&#93; The different CPUs often need to send and receive messages to each other;&#91;19&#93; to ensure good performance, the operating systems for these machines need to minimize this copying of packets.&#91;20&#93; Newer systems are often multiqueue—separating groups of users into separate queues—to reduce the need for packet copying and support more concurrent users.&#91;21&#93; Another technique is remote direct memory access, which enables each CPU to access memory belonging to other CPUs.&#91;19&#93; Multicomputer operating systems often support remote procedure calls where a CPU can call a procedure on another CPU,&#91;22&#93; or distributed shared memory, in which the operating system uses virtualization to generate shared memory that does not physically exist.&#91;23&#93;\n\nA distributed system is a group of distinct, networked computers—each of which might have their own operating system and file system. Unlike multicomputers, they may be dispersed anywhere in the world.&#91;24&#93; Middleware, an additional software layer between the operating system and applications, is often used to improve consistency. Although it functions similarly to an operating system, it is not a true operating system.&#91;25&#93;\n\nEmbedded operating systems are designed to be used in embedded computer systems, whether they are internet of things objects or not connected to a network. Embedded systems include many household appliances. The distinguishing factor is that they do not load user-installed software. Consequently, they do not need protection between different applications, enabling simpler designs. Very small operating systems might run in less than 10 kilobytes,&#91;26&#93;  and the smallest are for smart cards.&#91;27&#93;  Examples include Embedded Linux, QNX, VxWorks, and the extra-small systems RIOT and TinyOS.&#91;28&#93;\n\nA real-time operating system is an operating system that guarantees to process events or data by or at a specific moment in time. Hard real-time systems require exact timing and are common in manufacturing, avionics, military, and other similar uses.&#91;28&#93; With soft real-time systems, the occasional missed event is acceptable; this category often includes audio or multimedia systems, as well as smartphones.&#91;28&#93; In order for hard real-time systems be sufficiently exact in their timing, often they are just a library with no protection between applications, such as eCos.&#91;28&#93;\n\nA hypervisor is an operating system that runs a virtual machine. The virtual machine is unaware that it is an application and operates as if it had its own hardware.&#91;14&#93;&#91;29&#93; Virtual machines can be paused, saved, and resumed, making them useful for operating systems research, development,&#91;30&#93; and debugging.&#91;31&#93; They also enhance portability by enabling applications to be run on a computer even if they are not compatible with the base operating system.&#91;14&#93;\n\nA library operating system (libOS) is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with a single application and configuration code to construct a unikernel:\n&#91;32&#93; a specialized (only the absolute necessary pieces of code are extracted from libraries and bound together\n&#91;33&#93;), single address space, machine image that can be deployed to cloud or embedded environments.\n\nThe operating system code and application code are not executed in separated protection domains (there is only a single application running, at least conceptually, so there is no need to prevent interference between applications) and OS services are accessed via simple library calls (potentially inlining them based on compiler thresholds), without the usual overhead of context switches,\n&#91;34&#93; in a way similarly to embedded and real-time OSes. Note that this overhead is not negligible: to the direct cost of mode switching it's necessary to add the indirect pollution of important processor structures (like CPU caches, the instruction pipeline, and so on) which affects both user-mode and kernel-mode performance.\n&#91;35&#93;\n\nThe first computers in the late 1940s and 1950s were directly programmed either with plugboards or with machine code inputted on media such as punch cards, without programming languages or operating systems.&#91;36&#93; After the introduction of the transistor in the mid-1950s, mainframes began to be built. These still needed professional operators&#91;36&#93; who manually do what a modern operating system would do, such as scheduling programs to run,&#91;37&#93; but mainframes still had rudimentary operating systems such as Fortran Monitor System (FMS) and IBSYS.&#91;38&#93; In the 1960s, IBM introduced the first series of intercompatible computers (System/360). All of them ran the same operating system—OS/360—which consisted of millions of lines of assembly language that had thousands of bugs. The OS/360 also was the first popular operating system to support multiprogramming, such that the CPU could be put to use on one job while another was waiting on input/output (I/O). Holding multiple jobs in memory necessitated memory partitioning and safeguards against one job accessing the memory allocated to a different one.&#91;39&#93;\n\nAround the same time, teleprinters began to be used as terminals so multiple users could access the computer simultaneously. The operating system MULTICS was intended to allow hundreds of users to access a large computer. Despite its limited adoption, it can be considered the precursor to cloud computing. The UNIX operating system originated as a development of MULTICS for a single user.&#91;40&#93; Because UNIX's source code was available, it became the basis of other, incompatible operating systems, of which the most successful were AT&amp;T's System V and the University of California's Berkeley Software Distribution (BSD).&#91;41&#93; To increase compatibility, the IEEE released the POSIX standard for operating system application programming interfaces (APIs), which is supported by most UNIX systems. MINIX was a stripped-down version of UNIX, developed in 1987 for educational uses, that inspired the commercially available, free software Linux. Since 2008, MINIX is used in controllers of most Intel microchips, while Linux is widespread in data centers and Android smartphones.&#91;42&#93;\n\nThe invention of large scale integration enabled the production of personal computers (initially called microcomputers) from around 1980.&#91;43&#93; For around five years, the CP/M (Control Program for Microcomputers) was the most popular operating system for microcomputers.&#91;44&#93; Later, IBM bought the DOS (Disk Operating System) from Microsoft. After modifications requested by IBM, the resulting system was called MS-DOS (MicroSoft Disk Operating System) and was widely used on IBM microcomputers. Later versions increased their sophistication, in part by borrowing features from UNIX.&#91;44&#93;\n\nApple's Macintosh was the first popular computer to use a graphical user interface (GUI). The GUI proved much more user friendly than the text-only command-line interface earlier operating systems had used. Following the success of Macintosh, MS-DOS was updated with a GUI overlay called Windows. Windows later was rewritten as a stand-alone operating system, borrowing so many features from another (VAX VMS) that a large legal settlement was paid.&#91;45&#93; In the twenty-first century, Windows continues to be popular on personal computers but has less market share of servers. UNIX operating systems, especially Linux, are the most popular on enterprise systems and servers but are also used on mobile devices and many other computer systems.&#91;46&#93;\n\nOn mobile devices, Symbian OS was dominant at first, being usurped by BlackBerry OS (introduced 2002) and iOS for iPhones (from 2007). Later on, the open-source Android operating system (introduced 2008), with a Linux kernel and a C library (Bionic) partially based on BSD code, became most popular.&#91;47&#93;\n\nThe components of an operating system are designed to ensure that various parts of a computer function cohesively. With the de facto obsoletion of DOS, all user software must interact with the operating system to access hardware.\n\nThe kernel is the part of the operating system that provides protection between different applications and users. This protection is key to improving reliability by keeping errors isolated to one program, as well as security by limiting the power of malicious software and protecting private data, and ensuring that one program cannot monopolize the computer's resources.&#91;48&#93; Most operating systems have two modes of operation:&#91;49&#93;  in user mode, the hardware checks that the software is only executing legal instructions, whereas the kernel has unrestricted powers and is not subject to these checks.&#91;50&#93; The kernel also manages memory for other processes and controls access to input/output devices.&#91;51&#93;\n\nThe operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors.\n\nAn interrupt (also known as an abort, exception, fault, signal,&#91;52&#93; or trap)&#91;53&#93; provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR).&#91;54&#93;&#91;55&#93; An interrupt service routine may cause the central processing unit (CPU) to have a context switch.&#91;56&#93;&#91;a&#93; The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system.&#91;57&#93; However, several interrupt functions are common.&#91;57&#93; The architecture and operating system must:&#91;57&#93;\n\nA software interrupt is a message to a process that an event has occurred.&#91;52&#93; This contrasts with a hardware interrupt — which is a message to the central processing unit (CPU) that an event has occurred.&#91;58&#93; Software interrupts are similar to hardware interrupts — there is a change away from the currently running process.&#91;59&#93; Similarly, both hardware and software interrupts execute an interrupt service routine.\n\nSoftware interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch.&#91;60&#93; A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long.&#91;61&#93;\n\nSoftware interrupts may be error conditions, such as a malformed machine instruction.&#91;61&#93; However, the most common error conditions are division by zero and accessing an invalid memory address.&#91;61&#93;\n\nUsers can send messages to the kernel to modify the behavior of a currently running process.&#91;61&#93; For example, in the command-line environment, pressing the interrupt character (usually Control-C) might terminate the currently running process.&#91;61&#93;\n\nTo generate software interrupts for x86 CPUs, the INT assembly language instruction is available.&#91;62&#93; The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table.\n\nTo generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process.&#91;63&#93; pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format)&#91;b&#93; to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.)&#91;64&#93;\n\nIn Unix-like operating systems, signals inform processes of the occurrence of asynchronous events.&#91;63&#93; To communicate asynchronously, interrupts are required.&#91;65&#93; One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem.&#91;66&#93; The writer receives a pipe from the shell for its output to be sent to the reader's input stream.&#91;67&#93; The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue.&#91;68&#93; bravo will then be moved to the ready queue and soon will read from its input stream.&#91;69&#93; The kernel will generate software interrupts to coordinate the piping.&#91;69&#93;\n\nSignals may be classified into 7 categories.&#91;63&#93; The categories are:\n\nInput/output (I/O) devices are slower than the CPU. Therefore, it would slow down the computer if the CPU had to wait for each I/O to finish. Instead, a computer may implement interrupts for I/O completion, avoiding the need for polling or busy waiting.&#91;70&#93;\n\nSome computers require an interrupt for each character or word, costing a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly.&#91;71&#93; (Separate from the architecture, a device may perform direct memory access&#91;c&#93; to and from main memory either directly or via a bus.)&#91;72&#93;&#91;d&#93;\n\nWhen a computer user types a key on the keyboard, typically the character appears immediately on the screen. Likewise, when a user moves a mouse, the cursor immediately moves across the screen. Each keystroke and mouse movement generates an interrupt called Interrupt-driven I/O. An interrupt-driven I/O occurs when a process causes an interrupt for every character&#91;72&#93; or word&#91;73&#93; transmitted.\n\nDevices such as hard disk drives, solid-state drives, and magnetic tape drives can transfer data at a rate high enough that interrupting the CPU for every byte or word transferred, and having the CPU transfer the byte or word between the device and memory, would require too much CPU time. Data is, instead, transferred between the device and memory independently of the CPU by hardware such as a channel or a direct memory access controller; an interrupt is delivered only when all the data is transferred.&#91;74&#93;\n\nIf a computer program executes a system call to perform a block I/O write operation, then the system call might execute the following instructions:\n\nWhile the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus.&#91;78&#93; Upon accepting the interrupt request, the operating system will:\n\nWhen the writing process has its time slice expired, the operating system will:&#91;79&#93;\n\nWith the program counter now reset, the interrupted process will resume its time slice.&#91;57&#93;\n\nAmong other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.\n\nCooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen anymore, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.\n\nMemory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which does not exist in all computers.\n\nIn both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.\n\nWindows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.\n\nThe use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.\n\nIf a program tries to access memory that is not accessible&#91;e&#93; memory, but nonetheless has been allocated to it, the kernel is interrupted .mw-parser-output div.crossreference{padding-left:0}(see §&#160;Memory management). This kind of interrupt is typically a page fault.\n\nWhen the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has been allocated yet.\n\nIn modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.\n\nVirtual memory provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.&#91;80&#93;\n\nConcurrency refers to the operating system's ability to carry out multiple tasks simultaneously.&#91;81&#93; Virtually all modern operating systems support concurrency.&#91;82&#93;\n\nThreads enable splitting a process' work into multiple parts that can run simultaneously.&#91;83&#93; The number of threads is not limited by the number of processors available. If there are more threads than processors, the operating system kernel schedules, suspends, and resumes threads, controlling when each thread runs and how much CPU time it receives.&#91;84&#93;  During a context switch a running thread is suspended, its state is saved into the thread control block and stack, and the state of the new thread is loaded in.&#91;85&#93; Historically, on many systems a thread could run until it relinquished control (cooperative multitasking). Because this model can allow a single thread to monopolize the processor, most operating systems now can interrupt a thread (preemptive multitasking).&#91;86&#93;\n\nThreads have their own thread ID, program counter (PC), a register set, and a stack, but share code, heap data, and other resources with other threads of the same process.&#91;87&#93;&#91;88&#93; Thus, there is less overhead to create a thread than a new process.&#91;89&#93; On single-CPU systems, concurrency is switching between processes. Many computers have multiple CPUs.&#91;90&#93; Parallelism with multiple threads running on different CPUs can speed up a program, depending on how much of it can be executed concurrently.&#91;91&#93;\n\nPermanent storage devices used in twenty-first century computers, unlike volatile dynamic random-access memory (DRAM), are still accessible after a crash or power failure. Permanent (non-volatile) storage is much cheaper per byte, but takes several orders of magnitude longer to access, read, and write.&#91;92&#93;&#91;93&#93; The two main technologies are a hard drive consisting of magnetic disks, and flash memory (a solid-state drive that stores data in electrical circuits). The latter is more expensive but faster and more durable.&#91;94&#93;&#91;95&#93;\n\nFile systems are an abstraction used by the operating system to simplify access to permanent storage. They provide human-readable filenames and other metadata, increase performance via amortization of accesses, prevent multiple threads from accessing the same section of memory, and include checksums to identify corruption.&#91;96&#93; File systems are composed of files (named collections of data, of an arbitrary size) and directories (also called folders) that list human-readable filenames and other directories.&#91;97&#93; An absolute file path begins at the root directory and lists subdirectories divided by punctuation, while a relative path defines the location of a file from a directory.&#91;98&#93;&#91;99&#93;\n\nSystem calls (which are sometimes wrapped by libraries) enable applications to create, delete, open, and close files, as well as link, read, and write to them. All these operations are carried out by the operating system on behalf of the application.&#91;100&#93; The operating system's efforts to reduce latency include storing recently requested blocks of memory in a cache and prefetching data that the application has not asked for, but might need next.&#91;101&#93; Device drivers are software specific to each input/output (I/O) device that enables the operating system to work without modification over different hardware.&#91;102&#93;&#91;103&#93;\n\nAnother component of file systems is a dictionary that maps a file's name and metadata to the data block where its contents are stored.&#91;104&#93; Most file systems use directories to convert file names to file numbers. To find the block number, the operating system uses an index (often implemented as a tree).&#91;105&#93; Separately, there is a free space map to track free blocks, commonly implemented as a bitmap.&#91;105&#93; Although any free block can be used to store a new file, many operating systems try to group together files in the same directory to maximize performance, or periodically reorganize files to reduce fragmentation.&#91;106&#93;\n\nMaintaining data reliability in the face of a computer crash or hardware failure is another concern.&#91;107&#93; File writing protocols are designed with atomic operations so as not to leave permanent storage in a partially written, inconsistent state in the event of a crash at any point during writing.&#91;108&#93; Data corruption is addressed by redundant storage (for example, RAID—redundant array of inexpensive disks)&#91;109&#93;&#91;110&#93; and checksums to detect when data has been corrupted. With multiple layers of checksums and backups of a file, a system can recover from multiple hardware failures. Background processes are often used to detect and recover from data corruption.&#91;110&#93;\n\nSecurity means protecting users from other users of the same computer, as well as from those who seeking remote access to it over a network.&#91;111&#93;  Operating systems security rests on achieving the CIA triad: confidentiality (unauthorized users cannot access data), integrity (unauthorized users cannot modify data), and availability (ensuring that the system remains available to authorized users, even in the event of a denial of service attack).&#91;112&#93; As with other computer systems, isolating security domains—in the case of operating systems, the kernel, processes, and virtual machines—is key to achieving security.&#91;113&#93; Other ways to increase security include simplicity to minimize the attack surface, locking access to resources by default, checking all requests for authorization, principle of least authority (granting the minimum privilege essential for performing a task), privilege separation, and reducing shared data.&#91;114&#93;\n\nSome operating system designs are more secure than others. Those with no isolation between the kernel and applications are least secure, while those with a monolithic kernel like most general-purpose operating systems are still vulnerable if any part of the kernel is compromised. A more secure design features microkernels that separate the kernel's privileges into many separate security domains and reduce the consequences of a single kernel breach.&#91;115&#93; Unikernels are another approach that improves security by minimizing the kernel and separating out other operating systems functionality by application.&#91;115&#93;\n\nMost operating systems are written in C or C++, which create potential vulnerabilities for exploitation. Despite attempts to protect against them, vulnerabilities are caused by buffer overflow attacks, which are enabled by the lack of bounds checking.&#91;116&#93;  Hardware vulnerabilities, some of them caused by CPU optimizations, can also be used to compromise the operating system.&#91;117&#93; There are known instances of operating system programmers deliberately implanting vulnerabilities, such as back doors.&#91;118&#93;\n\nOperating systems security is hampered by their increasing complexity and the resulting inevitability of bugs.&#91;119&#93; Because formal verification of operating systems may not be feasible, developers use operating system hardening to reduce vulnerabilities,&#91;120&#93; e.g. address space layout randomization, control-flow integrity,&#91;121&#93; access restrictions,&#91;122&#93; and other techniques.&#91;123&#93; There are no restrictions on who can contribute code to open source operating systems; such operating systems have transparent change histories and distributed governance structures.&#91;124&#93; Open source developers strive to work collaboratively to find and eliminate security vulnerabilities, using code review and type checking to expunge malicious code.&#91;125&#93;&#91;126&#93; Andrew S. Tanenbaum advises releasing the source code of all operating systems, arguing that it prevents developers from placing trust in secrecy and thus relying on the unreliable practice of security by obscurity.&#91;127&#93;\n\nA user interface (UI) is essential to support human interaction with a computer. The two most common user interface types for any computer are\n\nFor personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software.&#91;128&#93; Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers.&#91;129&#93; The software to support GUIs is more complex than a command line for input and plain text output. Plain text output is often preferred by programmers, and is easy to support.&#91;130&#93;\n\nA hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.&#91;131&#93;\n\nIn some cases, hobby development is in support of a \"homebrew\" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.\n\nExamples of hobby operating systems include Syllable and TempleOS.\n\nIf an application is written for use on a specific operating system, and is ported to another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.\n\nThis cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.\n\nAnother approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.\n\nAs of September&#160;2024&#91;update&#93;, Android is the most popular operating system with a 46% market share, followed by Microsoft Windows at 26%, iOS and iPadOS at 18%, macOS at 5%, and Linux at 1%. Android, iOS, and iPadOS are mobile operating systems, while Windows, macOS, and Linux are desktop operating systems.&#91;3&#93;\n\nLinux is a free software distributed under the GNU General Public License (GPL), which means that all of its derivatives are legally required to release their source code.&#91;132&#93; Linux was designed by programmers for their own use, thus emphasizing simplicity and consistency, with a small number of basic elements that can be combined in nearly unlimited ways, and avoiding redundancy.&#91;133&#93;\n\nIts design is similar to other UNIX systems not using a microkernel.&#91;134&#93; It is written in C&#91;135&#93; and uses UNIX System V syntax, but also supports BSD syntax. Linux supports standard UNIX networking features, as well as the full suite of UNIX tools, while supporting multiple users and employing preemptive multitasking. Initially of a minimalist design, Linux is a flexible system that can work in under 16 MB of RAM, but still is used on large multiprocessor systems.&#91;134&#93; Similar to other UNIX systems, Linux distributions are composed of a kernel, system libraries, and system utilities.&#91;136&#93; Linux has a graphical user interface (GUI) with a desktop, folder and file icons, as well as the option to access the operating system via a command line.&#91;137&#93;\n\nAndroid is a partially open-source operating system closely based on Linux and has become the most widely used operating system by users, due to its popularity on smartphones and, to a lesser extent, embedded systems needing a GUI, such as \"smart watches, automotive dashboards, airplane seatbacks, medical devices, and home appliances\".&#91;138&#93; Unlike Linux, much of Android is written in Java and uses object-oriented design.&#91;139&#93;\n\nWindows is a proprietary operating system that is widely used on desktop computers, laptops, tablets, phones, workstations, enterprise servers, and Xbox consoles.&#91;141&#93; The operating system was designed for \"security, reliability, compatibility, high performance, extensibility, portability, and international support\"—later on, energy efficiency and support for dynamic devices also became priorities.&#91;142&#93;\n\nWindows Executive works via kernel-mode objects for important data structures like processes, threads, and sections (memory objects, for example files).&#91;143&#93; The operating system supports demand paging of virtual memory, which speeds up I/O for many applications. I/O device drivers use the Windows Driver Model.&#91;143&#93; The NTFS file system has a master table and each file is represented as a record with metadata.&#91;144&#93; The scheduling includes preemptive multitasking.&#91;145&#93; Windows has many security features;&#91;146&#93; especially important are the use of access-control lists and integrity levels. Every process has an authentication token and each object is given a security descriptor. Later releases have added even more security features.&#91;144&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Operating_system"
  },
  {
    "title": "Atal Bihari Vajpayee",
    "content": "\n\n\n\n\nPremiership\n\n\n\nAtal Bihari Vajpayee&#91;1&#93; (25 December 1924&#160;–&#160;16 August 2018) was an Indian politician, poet, and statesman who served as the prime minister of India, first for a term of 13&#160;days in 1996, then for a period of 13&#160;months from 1998 to 1999, followed by a full term from 1999 to 2004.&#91;2&#93; He was the first non-Congress prime minister to serve a full term in the office. Vajpayee was one of the co-founders and a senior leader of the Bharatiya Janata Party (BJP). He was a member of the Rashtriya Swayamsevak Sangh (RSS), a far-right Hindu nationalist paramilitary volunteer organisation. He was also a Hindi poet and a writer.&#91;3&#93;&#91;4&#93;\n\nHe was a member of the Indian Parliament for over five decades, having been elected ten times to the Lok Sabha, the lower house, and twice to the Rajya Sabha, the upper house. He served as the Member of Parliament from Lucknow, Gwalior, New Delhi and Balrampur constituencies, before retiring from active politics in 2009 due to health concerns. He was among the founding members of the Bharatiya Jana Sangh (BJS), of which he was president from 1968 to 1972. The BJS merged with several other parties to form the Janata Party, which won the 1977 general election. In March 1977, Vajpayee became the minister of external affairs in the cabinet of Prime Minister Morarji Desai. He resigned in 1979, and the Janata alliance collapsed soon after. Former members of the Bharatiya Jana Sangh formed the BJP in 1980, with Vajpayee as its first president.\n\nDuring his tenure as prime minister, India carried out the Pokhran-II nuclear tests in 1998. Vajpayee sought to improve diplomatic relations with Pakistan, travelling to Lahore by bus to meet with Prime Minister, Nawaz Sharif. After the 1999 Kargil War with Pakistan, he sought to restore relations through engagement with President Pervez Musharraf, inviting him to India for a summit at Agra. Vajpayee's government introduced many domestic economic and infrastructural reforms, including encouraging the private sector and foreign investments, reducing governmental waste, encouraging research and development, and the privatisation of some government owned corporations.&#91;5&#93; During his tenure, India's security was threatened by a number of violent incidents including 2001 Indian Parliament attack and 2002 Gujarat riots which ultimately caused his defeat in 2004 general election.\n\nVajpayee was conferred with the Padma Vibhushan in 1992, India's second highest civilian award by the Government of India. The administration of Narendra Modi declared in 2014 that Vajpayee's birthday, 25&#160;December would be marked as Good Governance Day. In 2015, he was honoured India's highest civilian honour - Bharat Ratna, by the then President of India, Pranab Mukherjee. He died in 2018 due to age-related illness.\n\nVajpayee was born into a Kanyakubja Brahmin family on 25&#160;December 1924 in Gwalior, Madhya Pradesh.&#91;6&#93;&#91;7&#93; His mother was Krishna Devi and his father was Krishna Bihari Vajpayee.&#91;8&#93; His father was a school teacher in Gwalior.&#91;9&#93; His grandfather, Shyam Lal Vajpayee, had migrated to Morena, Madhya Pradesh from Bateshwar Uttar Pradesh.&#91;8&#93; Later he shifted to Gwalior from Morena for better opportunities.\n\nVajpayee did his primary schooling at the Saraswati Shishu Mandir, Gwalior and high school education from the Gorkhi School, Gwalior.&#91;10&#93; He subsequently attended Gwalior's Victoria College, (now Maharani Laxmi Bai Govt. College of Excellence) where he graduated with a Bachelor of Arts in Hindi, English and Sanskrit. Later for master's degree the Scindia dynasty of erstwhile Gwalior state sanctioned him monthly scholarship of Rs 75 and with this scholarship support he completed his post-graduation with a Master of Arts in political science from DAV College, Kanpur, Agra University.&#91;7&#93;&#91;11&#93;&#91;12&#93;\n\nHis activism started in Gwalior with Arya Kumar Sabha, the youth wing of the Arya Samaj movement, of which he became the general secretary in 1944. He also joined the Rashtriya Swayamsevak Sangh (RSS) in 1939 as a swayamsevak, or volunteer in Gwalior at the age of 12 years. Influenced by Babasaheb Apte, he attended the Officers Training Camp of the RSS during 1940 to 1944, becoming a pracharak (RSS terminology for a full-time worker) in 1947. He gave up studying law due to the partition riots. He was sent to Uttar Pradesh as a vistarak (a probationary pracharak) and soon began working for the newspapers of Deendayal Upadhyaya: Rashtradharma (a Hindi monthly), Panchjanya (a Hindi weekly), and the dailies Swadesh and Veer Arjun.&#91;11&#93;&#91;13&#93;&#91;14&#93;\n\nBy 1942, at the age of 16 years, Vajpayee became an active member of the Rashtriya Swayamsevak Sangh (RSS) and joined it shakha in Gwalior along with his elder brother. Although the RSS had chosen not to participate in the Quit India Movement, in August 1942, Vajpayee and his elder brother Prem were arrested for 24 days during the Quit India Movement. He was released after giving a written statement that while he was a part of the crowd, he did not participate in the militant events in Bateshwar on 27 August 1942. Throughout his life, including after he became prime minister, Vajpayee has labelled the allegation of participation in the Quit India Movement to be a false rumour.&#91;15&#93;\n\nIn 1951, Vajpayee was seconded by the RSS, along with Deendayal Upadhyaya, to work for the newly formed Bharatiya Jana Sangh, a Hindu right-wing political party associated with the RSS. He was appointed as a national secretary of the party in charge of the Northern region, based in Delhi. He soon became a follower and aide of party leader Syama Prasad Mukherjee. In the 1957 Indian general election, Vajpayee contested elections to the Lok Sabha, the lower house of the Indian Parliament. He lost to Raja Mahendra Pratap in Mathura, but was elected from Balrampur.\n\nHe was influenced by Jawaharlal Nehru to the extent that he mirrored his style, diction, and tone of his speeches.&#91;16&#93;&#91;17&#93; Nehru's influence was also evident in Vajpayee's leadership.&#91;18&#93; In the Lok Sabha his oratorial skills so impressed Prime Minister Nehru that he predicted that Vajpayee would someday become the prime minister of India.&#91;19&#93;&#91;20&#93;&#91;21&#93; On the occasion of Nehru's death on 27 May 1964, Vajpayee termed him as \"the orchestrator of the impossible and inconceivable\" and likened him to Hindu god Rama.&#91;22&#93;&#91;23&#93;\n\nVajpayee's oratorial skills won him the reputation of being the most eloquent defender of the Jana Sangh's policies.&#91;24&#93; After the death of Upadhyaya, the leadership of the Jana Sangh passed to Vajpayee.&#91;25&#93; He became the national president of the Jana Sangh in 1968,&#91;26&#93; running the party along with Nanaji Deshmukh, Balraj Madhok and L. K. Advani.&#91;25&#93;\n\nVajpayee was arrested along with several other opposition leaders during the Internal Emergency imposed by Prime Minister Indira Gandhi in 1975.&#91;9&#93;&#91;27&#93; Initially interned in Bangalore, Vajpayee appealed his imprisonment on the grounds of bad health, and was moved to a hospital in Delhi.&#91;28&#93; In December 1976, Vajpayee ordered the student activists of the ABVP to tender an unconditional apology to Indira Gandhi for perpetrating violence and disorder.&#91;29&#93;&#91;30&#93; The ABVP student leaders refused to obey his order.&#91;29&#93;&#91;31&#93;\n\nGandhi ended the state of emergency in 1977.&#91;32&#93; A coalition of parties, including the BJS, came together to form the Janata Party, which won the 1977 general elections.&#91;33&#93; Morarji Desai, the chosen leader of the alliance, became the prime minister. Vajpayee served as the minister of external affairs, or foreign minister, in Desai's cabinet.&#91;34&#93; As foreign minister, Vajpayee became the first person in 1977 to deliver a speech to the United Nations General Assembly in Hindi.&#91;34&#93;\n\nIn 1979, Desai and Vajpayee resigned, triggering the collapse of the Janata Party.&#91;28&#93;&#91;35&#93; The erstwhile members of the Bharatiya Jana Sangh came together to form the Bharatiya Janata Party (BJP) in 1980, with Vajpayee as its first President.&#91;36&#93;\n\nLeading up to Operation Bluestar, there were several protests by Sangh Parivar, including a march led by LK Advani and Vajpayee of the Bhartiya Janta Party to protest against the lack of government action and to demand that the Indian Army be sent into the Golden Temple.&#91;37&#93;&#91;38&#93;\n\nThe 1984 general elections were held in the wake of Prime Minister Indira Gandhi's assassination by her Sikh bodyguards. While he had won the 1977 and the 1980 elections from New Delhi, Vajpayee shifted to his home town Gwalior for the election.&#91;39&#93;\n\nVidya Razdan was initially tipped to be the Congress candidate. Instead, Madhavrao Scindia, scion of the Gwalior royal family, was brought in on the last day of filing nominations.&#91;40&#93; Vajpayee lost to Scindia, managing to secure only 29% of the votes.&#91;39&#93;\n\nUnder Vajpayee, the BJP moderated the Hindu-nationalist position of the Jana Sangh, emphasising its connection to the Janata Party and expressing support for Gandhian Socialism.&#91;41&#93; The ideological shift did not bring it success and Indira Gandhi's assassination generated sympathy for the Congress, leading to a massive victory at the polls. The BJP won only two seats in parliament.&#91;41&#93; Vajpayee offered to quit as party president following BJP's dismal performance in the election,&#91;42&#93; but stayed in the post until 1986.&#91;43&#93; He was elected to the Rajya Sabha in 1986 from Madhya Pradesh,&#91;44&#93; and was briefly the leader of the BJP in Parliament.&#91;45&#93;\n\nIn 1986, L. K. Advani took office as president of the BJP.&#91;46&#93; Under him, the BJP returned to a policy of hardline Hindu nationalism.&#91;41&#93; It became the political voice of the Ram Janmabhoomi Mandir Movement, which sought to build a temple dedicated to the Hindu deity Rama in Ayodhya. The temple would be built at a site believed to be the birthplace of Rama after demolishing a 16th-century mosque, called the Babri Masjid, which then stood there.&#91;47&#93; The strategy paid off for the BJP; it won 86 seats in the Lok Sabha in the 1989 general election, making its support crucial to the government of V. P. Singh.&#91;41&#93; In December 1992, a group of religious volunteers led by members of the BJP, the Rashtriya Swayamsevak Sangh (RSS) and the Vishwa Hindu Parishad (VHP), tore down the mosque.&#91;48&#93;&#91;24&#93;\n\nHe served as Member of Parliament, Lok Sabha, for various terms starting at Balrampur from 1957–1962. He served again from Balrampur from 1967–1971, then from Gwalior from 1971–1977, and then from New Delhi from 1977–1984. Finally, he served from Lucknow from 1991–2009.&#91;49&#93;\n\nDuring a BJP conference in Mumbai in November 1995, BJP President Advani declared that Vajpayee would be the party's prime ministerial candidate in the forthcoming elections. Vajpayee himself was reported to be unhappy with the announcement, responding by saying that the party needed to win the election first.&#91;50&#93; The BJP became the single largest party in Parliament in the 1996 general election, helped by religious polarisation across the country as a result of the demolition of the Babri Masjid.&#91;51&#93;&#91;52&#93; Indian president Shankar Dayal Sharma invited Vajpayee to form the government.&#91;53&#93; Vajpayee was sworn in as the 10th prime minister of India,&#91;54&#93; but the BJP failed to muster a majority among members of the Lok Sabha. Vajpayee resigned after 16 days, when it became clear that he did not have enough support to form a government.&#91;54&#93;&#91;55&#93; In this short period, he also created and administered the Ministry of Consumer Affairs, Food and Public Distribution.\n\nAfter the fall of the two United Front governments between 1996 and 1998, the Lok Sabha was dissolved and fresh elections were held. The 1998 general elections again put the BJP ahead of others. A number of political parties joined the BJP to form the National Democratic Alliance (NDA), and Vajpayee was sworn in as the prime minister.&#91;56&#93; The coalition was an uneasy one,&#91;33&#93; as apart from the Shiv Sena, none of the other parties espoused the BJP's Hindu-nationalist ideology.&#91;57&#93; Vajpayee has been credited for managing this coalition successfully, while facing ideological pressure from the hardline wing of the party and from the RSS.&#91;28&#93; Vajpayee's government lasted 13 months until mid-1999 when the All India Anna Dravida Munnetra Kazhagam (AIADMK) under J. Jayalalithaa withdrew its support.&#91;58&#93; The government lost the ensuing vote of confidence motion in the Lok Sabha by a single vote on 17 April 1999.&#91;59&#93; As the opposition was unable to come up with the numbers to form the new government, the Lok Sabha was again dissolved and fresh elections were held.&#91;60&#93;\n\nIn May 1998, India conducted five underground nuclear tests in the Pokhran desert in Rajasthan, 24 years after its first nuclear test, operation Smiling Buddha in 1974. Two weeks later, Pakistan responded with its own nuclear tests making it the newest nation with declared nuclear capability.&#91;61&#93; While some nations, such as France, endorsed India's right to defensive nuclear power,&#91;62&#93; others including the United States, Canada, Japan, Britain and the European Union imposed sanctions on information, resources and technology to India. In spite of intense international criticism and steady decline in foreign investment and trade, the nuclear tests were popular domestically. In effect, the international sanctions imposed failed to sway India from weaponising its nuclear capability. US sanctions against India and Pakistan were eventually lifted after just six months.&#91;63&#93;\n\nIn late 1998 and early 1999, Vajpayee began a push for a full-scale diplomatic peace process with Pakistan. With the historic inauguration of the Delhi-Lahore bus service in February 1999, Vajpayee initiated a new peace process aimed towards permanently resolving the Kashmir dispute and other conflicts with Pakistan. The resultant Lahore Declaration espoused a commitment to dialogue, expanded trade relations and mutual friendship and envisaged a goal of denuclearised South Asia. This eased the tension created by the 1998 nuclear tests, not only within the two nations but also in South Asia and the rest of the world.&#91;64&#93;&#91;65&#93;\n\nThe AIADMK had continually threatened to withdraw from the coalition and national leaders repeatedly flew down from Delhi to Chennai to pacify the AIADMK general secretary J. Jayalalithaa. However, in May 1999, the AIADMK withdrew from NDA, and the Vajpayee administration was reduced to a caretaker status pending fresh elections scheduled for October 1999.&#91;66&#93;\n\nIn May 1999 some Kashmiri shepherds discovered the presence of militants and non-uniformed Pakistani soldiers (many with official identifications and Pakistan Army's custom weaponry) in the Kashmir Valley, where they had taken control of border hilltops and unmanned border posts. The incursion was centred around the town of Kargil, but also included the Batalik and Akhnoor sectors and artillery exchanges at the Siachen Glacier.&#91;67&#93;&#91;68&#93;\n\nThe Indian army responded with Operation Vijay, which launched on 26 May 1999. This saw the Indian military fighting thousands of militants and soldiers in the midst of heavy artillery shelling and while facing extremely cold weather, snow and treacherous terrain at the high altitude.&#91;69&#93; Over 500 Indian soldiers were killed in the three-month-long Kargil War, and it is estimated around 600–4,000 Pakistani militants and soldiers died as well.&#91;70&#93;&#91;71&#93;&#91;72&#93;&#91;73&#93; India pushed back the Pakistani militants and Northern Light Infantry soldiers. Almost 70% of the territory was recaptured by India.&#91;69&#93; Vajpayee sent a \"secret letter\" to U.S. President Bill Clinton that if Pakistani infiltrators did not withdraw from the Indian territory, \"we will get them out, one way or the other\".&#91;74&#93;\n\nAfter Pakistan suffered heavy losses, and with both the United States and China refusing to condone the incursion or threaten India to stop its military operations, General Pervez Musharraf was recalcitrant and Nawaz Sharif asked the remaining militants to stop and withdraw to positions along the LoC.&#91;75&#93; The militants were not willing to accept orders from Sharif but the NLI soldiers withdrew.&#91;75&#93; The militants were killed by the Indian army or forced to withdraw in skirmishes which continued even after the announcement of withdrawal by Pakistan.&#91;75&#93;\n\nThe 1999 general elections were held in the aftermath of the Kargil operations. The BJP-led NDA won 303 seats out of the 543 seats in the Lok Sabha, securing a comfortable and stable majority.&#91;76&#93; On 13 October 1999, Vajpayee took oath as the prime minister of India for the third time.&#91;77&#93;\n\nA national crisis emerged in December 1999, when Indian Airlines flight IC 814 from Kathmandu to New Delhi was hijacked by five terrorists and flown to Taliban-ruled Afghanistan.&#91;78&#93; The hijackers made several demands including the release of certain terrorists like Masood Azhar from prison. Under pressure, the government ultimately caved in. Jaswant Singh, the then minister of external affairs, flew with the terrorists to Afghanistan and exchanged them for the passengers.&#91;79&#93;\n\nIn March 2000, Bill Clinton, the President of the United States, paid a state visit to India.&#91;80&#93; This was the first state visit to India by a U.S. president in 22 years, since President Jimmy Carter's visit in 1978.&#91;81&#93; President Clinton's visit was hailed as a significant milestone in relations between the two nations.&#91;80&#93; Vajpayee and Clinton had wide-ranging discussions on bilateral, regional and international developments.&#91;82&#93; The visit led to expansion in trade and economic ties between India and the United States.&#91;83&#93; A vision document on the future course of Indo-U.S. relations was signed during the visit.&#91;84&#93;\n\nDomestically, the BJP-led government was influenced by the RSS, but owing to its dependence on coalition support, it was impossible for the BJP to push items like building the Ram Janmabhoomi temple in Ayodhya, repealing Article 370 which gave a special status to the state of Kashmir, or enacting a uniform civil code applicable to adherents of all religions. On 17 January 2000, there were reports of the RSS and some BJP hard-liners threatening to restart the Jan Sangh, the precursor to the BJP, because of their discontent over Vajpayee's rule. Former president of the Jan Sangh Balraj Madhok had written a letter to the then-RSS chief Rajendra Singh for support.&#91;85&#93; The BJP was, however, accused of \"saffronising\" the official state education curriculum and apparatus, saffron being the colour of the RSS flag of the RSS, and a symbol of the Hindu nationalism movement.&#91;86&#93; Home Minister L. K. Advani and the Human Resource Development Minister (now called Education Minister)&#91;87&#93; Murli Manohar Joshi were indicted in the 1992 Babri Mosque demolition case for inciting a mob of activists. Vajpayee himself came under public scrutiny owing to his controversial speech one day prior to the mosque demolition.&#91;88&#93;\n\nThese years were accompanied by infighting in the administration and confusion regarding the direction of government.&#91;89&#93;&#91;90&#93; Vajpayee's weakening health was also a subject of public interest, and he underwent a major knee-replacement surgery at the Breach Candy Hospital in Mumbai to relieve intense pressure upon his legs.&#91;91&#93;\n\nIn March 2001, the Tehelka group released a sting operation video named Operation West End which showed BJP president Bangaru Laxman, senior army officers and NDA members accepting bribes from journalists posing as agents and businessmen.&#91;92&#93;&#91;93&#93; The Defence Minister George Fernandes was forced to resign following the Barak Missile scandal involving the botched supplies of coffins for the soldiers killed in Kargil, and the findings of an inquiry commission that the government could have prevented the Kargil invasion.&#91;94&#93;\n\nVajpayee initiated talks with Pakistan and invited Pakistani president Pervez Musharraf to Agra for a joint summit. President Musharraf was believed to be the principal architect of the Kargil War in India.&#91;95&#93; By accepting him as the President of Pakistan, Vajpayee chose to move forward leaving behind the Kargil War. But after three days of much fanfare, which included Musharraf visiting his birthplace in Delhi, the summit failed to achieve a breakthrough as President Musharraf declined to leave aside the issue of Kashmir.&#91;96&#93;\n\nOn 13 December 2001, a group of masked, armed men with fake IDs stormed Parliament House in Delhi.&#91;97&#93; The terrorists managed to kill several security guards, but the building was sealed off swiftly and security forces cornered and killed the men who were later proven to be Pakistan nationals.&#91;98&#93; Vajpayee ordered Indian troops to mobilise for war, leading to an estimated 500,000&#91;99&#93; to 750,000&#91;100&#93; Indian soldiers positioned along the international border between India and Pakistan under Operation Parakram. Pakistan responded by mobilising its own troops along the border leading to the 2001-2002 military standoff.&#91;99&#93; A terrorist attack on an army garrison in Kashmir in May 2002 further escalated the situation. As the threat of war between two nuclear capable countries and the consequent possibility of a nuclear exchange loomed large, international diplomatic mediation focused on defusing the situation.&#91;101&#93; In October 2002, both India and Pakistan announced that they would withdraw their troops from the border.&#91;100&#93;\n\nThe Vajpayee administration brought in the Prevention of Terrorism Act in 2002. The act was aimed at curbing terrorist threats by strengthening powers of government authorities to investigate and act against suspects.&#91;102&#93;&#91;103&#93; It was passed in a joint session of the parliament, amidst concerns that the law would be misused.&#91;104&#93;\n\nAnother political disaster hit his government between December 2001 and March 2002 with the VHP and the Government engaging in a major standoff in Ayodhya over the Ram temple. On the 10th anniversary of the destruction of the Babri mosque, the VHP wanted to perform a shila daan, or a ceremony laying the foundation stone of the cherished temple at the disputed site.&#91;105&#93; Thousands of VHP activists amassed and threatened to overrun the site and forcibly perform the ceremony.&#91;106&#93;&#91;107&#93; A threat of communal violence and breakdown of law and order owing to the defiance of the government by a religious organisation hung over the nation. The incident, however, ended peacefully with a symbolic handover of a stone at a different location 1&#160;km away from the disputed site.&#91;108&#93;\n\nIn February 2002, a train filled with Hindu pilgrims returning to Gujarat from Ayodhya stopped in the town of Godhra. A scuffle broke out between Hindu activists and Muslim residents, and the train was set on fire, leading to the deaths of 59 people. The charred bodies of the victims were displayed in public in the city of Ahmedabad, and the Vishwa Hindu Parishad called for a statewide strike in Gujarat. These decisions stoked anti-Muslim sentiments.&#91;109&#93; Blaming Muslims for the deaths, rampaging Hindu mobs killed thousands of Muslim men and women, destroying Muslim homes and places of worship. The violence raged for more than two months, and more than 1,000 people died.&#91;110&#93; Gujarat was being ruled by a BJP government, with Narendra Modi as the chief minister. The state government was criticised for mishandling the situation.&#91;111&#93; It was accused of doing little to stop the violence, and even being complicit in encouraging it.&#91;112&#93;&#91;110&#93;\n\nVajpayee reportedly wanted to remove Modi but was eventually prevailed upon by party members to not act against him.&#91;113&#93;&#91;114&#93; He travelled to Gujarat, visiting Godhra, and Ahmedabad, the site of the most violent riots. He announced financial aid for victims and urged an end to the violence.&#91;115&#93; While he condemned the violence,&#91;116&#93; he did not chastise Modi directly in public. When asked as to what his message to the chief minister in the event of the riots would be, Vajpayee responded that Modi must follow raj dharma, Hindi for ethical governance.&#91;115&#93;\n\nAt the meeting of the BJP national executive in Goa in April 2002, Vajpayee's speech generated controversy for its contents which included him saying: \"Wherever Muslims live, they don't like to live in co-existence with others.\"&#91;117&#93;&#91;118&#93; The Prime Minister's Office stated that these remarks had been taken out of context.&#91;119&#93; Vajpayee was accused of doing nothing to stop the violence, and later admitted mistakes in handling the events.&#91;120&#93; K. R. Narayanan, then president of India, also blamed Vajpayee's government for failing to quell the violence.&#91;121&#93; After the BJP's defeat in the 2004 general elections, Vajpayee admitted that not removing Modi had been a mistake.&#91;122&#93;\n\nIn late 2002 and 2003 the government pushed through economic reforms.&#91;123&#93; The country's GDP growth exceeded 7% every year from 2003 to 2007, following three years of sub-5% growth.&#91;124&#93; Increasing foreign investment,&#91;123&#93; modernisation of public and industrial infrastructure, the creation of jobs, a rising high-tech and IT industry and urban modernisation and expansion improved the nation's international image. Good crop harvests and strong industrial expansion also helped the economy.&#91;125&#93;\n\nIn May 2003, he announced before the parliament that he would make one last effort to achieve peace with Pakistan. The announcement ended a period of 16 months, following the 2001 attack on the Indian parliament, during which India had severed diplomatic ties with Pakistan.&#91;126&#93; Although diplomatic relations did not pick up immediately, visits were exchanged by high-level officials and the military standoff ended. The Pakistani President and Pakistani politicians, civil and religious leaders hailed this initiative as did the leaders of the United States, Europe and much of the world. In July 2003, Prime Minister Vajpayee visited China and met with various Chinese leaders. He recognised Tibet as a part of China, which was welcomed by the Chinese leadership, and which, in the following year, recognised Sikkim as part of India. China–India relations improved greatly in the following years.&#91;127&#93;\n\nVajpayee's government introduced many domestic economic and infrastructural reforms, including encouraging the private sector and foreign investments, reducing governmental waste, encouraging research and development and privatisation of some government owned corporations.&#91;5&#93; Among Vajpayee's projects were the National Highways Development Project and Pradhan Mantri Gram Sadak Yojana.&#91;128&#93;&#91;129&#93; In 2001, the Vajpayee government launched the Sarva Shiksha Abhiyan campaign, aimed at improving the quality of education in primary and secondary schools.&#91;130&#93;&#91;131&#93;\n\nIn 2003, news reports suggested a tussle within the BJP with regard to sharing of leadership between Vajpayee and Deputy Prime Minister LK Advani.&#91;132&#93;&#91;133&#93; BJP president Venkaiah Naidu had suggested that Advani must lead the party politically at the 2004 general elections, referring to Vajpayee as vikas purush, Hindi for development man, and Advani as loh purush, iron man.&#91;134&#93; When Vajpayee subsequently threatened retirement, Naidu backtracked, announcing that the party would contest the elections under the twin leadership of Vajpayee and Advani.&#91;135&#93;\n\nThe NDA was widely expected to retain power after the 2004 general election. It announced elections six months ahead of schedule, hoping to capitalise on economic growth, and Vajpayee's peace initiative with Pakistan.&#91;136&#93;&#91;137&#93; The 13th Lok Sabha was dissolved before the completion of its term. The BJP hoped to capitalise on a perceived 'feel-good factor' and BJP's recent successes in the Assembly elections in Rajasthan, Madhya Pradesh and Chhattisgarh. Under the \"India Shining\" campaign, it released ads proclaiming the economic growth of the nation under the government.&#91;138&#93;&#91;139&#93;\n\nHowever, the BJP could only win 138 seats in the 543-seat parliament,&#91;140&#93; with several prominent cabinet ministers being defeated.&#91;137&#93; The NDA coalition won 185 seats. The Indian National Congress, led by Sonia Gandhi, emerged as the single largest party, winning 145 seats in the election. The Congress and its allies, comprising many smaller parties, formed the United Progressive Alliance, accounting for 220 seats in the parliament.&#91;140&#93; Vajpayee resigned as prime minister.&#91;141&#93; The UPA, with the outside support of communist parties, formed the next government with Manmohan Singh as the prime minister.&#91;142&#93;\n\nIn December 2005, Vajpayee announced his retirement from active politics, declaring that he would not contest in the next general election. In a famous statement at the BJP's silver jubilee rally at Mumbai's Shivaji Park, Vajpayee announced that \"Henceforth, Lal Krishna Advani and Pramod Mahajan will be the Ram-Lakshman [the two godly brothers much revered and worshipped by Hindus of the BJP.\"&#91;143&#93;\n\nVajpayee was referred to as the Bhishma Pitamah of Indian politics by former prime minister Manmohan Singh during a speech in the Rajya Sabha, a reference to the character in the Hindu epic Mahabharata who was held in respect by two warring sides.&#91;144&#93;\n\nVajpayee was hospitalised at All India Institute of Medical Sciences, Delhi (AIIMS) for a chest infection and fever on 6 February 2009. He was put on ventilator support as his condition worsened but he eventually recuperated and was later discharged.&#91;145&#93; Unable to participate in the campaign for the 2009 general election due to his poor health, he wrote a letter urging voters to back the BJP.&#91;146&#93; His protege Lalji Tandon was able to retain the Lucknow seat in that election even though the NDA suffered electoral reverses all over the country. It was speculated that Vajpayee's non-partisan appeal contributed to Lalji's success in Lucknow in contrast to that BJP's poor performance elsewhere in Uttar Pradesh.&#91;147&#93;\n\nMember, Business Advisory Committee\n\nNational Democratic Alliance (India)\n\nVajpayee remained a bachelor for his entire life.&#91;148&#93; He adopted and raised Namita Bhattacharya as his own child, the daughter of longtime friend Rajkumari Kaul and her husband B. N. Kaul. His adopted family lived with him.&#91;149&#93;\n\nUnlike purist Brahmins who shun meat and alcohol, Vajpayee was known to be fond of whisky and meat.&#91;150&#93;&#91;151&#93; He was a noted poet, writing in Hindi. His published works include Kaidi Kaviraj Ki Kundalian, a collection of poems written during the 1975–1977 emergency, and Amar aag hai.&#91;152&#93; With regard to his poetry he wrote\n\n\"My poetry is a declaration of war, not an exordium to defeat. It is not the defeated soldier's drumbeat of despair, but the fighting warrior's will to win. It is not the despirited voice of dejection but the stirring shout of victory.\"&#91;153&#93; \nVajpayee had a stroke in 2009 which impaired his speech.&#91;154&#93; His health had been a major source of concern; reports said he was reliant on a wheelchair and failed to recognise people. He also had dementia and long-term diabetes. For many years, he had not attended any public engagements and rarely ventured out of the house, except for checkups at the All India Institutes of Medical Sciences.&#91;154&#93;&#91;155&#93;\n\nOn 11 June 2018, Vajpayee was admitted to AIIMS in critical condition following a kidney infection.&#91;156&#93;&#91;157&#93; He was officially declared dead there at 5:05&#160;pm IST on 16 August 2018 at the age of 93.&#91;158&#93;&#91;159&#93; Some sources claim that he had died on the previous day.&#91;160&#93;&#91;161&#93; A seven-day state mourning was announced by the central government throughout India. The national flag flew half-mast during this period.&#91;162&#93;\n\nVajpayee authored several works of both Hindi poetry and prose. Some of his major publications are listed below. In addition to these, various collections were made of his speeches, articles, and slogans.&#91;171&#93;&#91;172&#93;&#91;173&#93;\n\nAn English translation of a selection of some of Vajpayee's Hindi poetry was published in 2013.&#91;189&#93;\n\nThe administration of Narendra Modi declared in 2014 that Vajpayee's birthday, 25&#160;December, would be marked as Good Governance Day.&#91;190&#93;&#91;191&#93; The world's longest tunnel, Atal Tunnel at Rohtang, Himachal Pradesh, on the Leh-Manali Highway was named after Atal Bihari Vajpayee.&#91;192&#93; The third longest cable-stayed bridge in India over the Mandovi River, Atal Setu was named in his memory.&#91;193&#93; The Government of Chhattisgarh changed the name of Naya Raipur to Atal Nagar.&#91;194&#93;\n\nThe Films Division of India has produced the short documentary films Pride of India Atal Bihari Vajpayee (1998) and Know Your Prime Minister Atal Behari Vajpayee (2003), both directed by Girish Vaidya, which explore different facets of his personality.&#91;195&#93;&#91;196&#93; Vajpayee also appears in a cameo in the 1977 Indian Hindi-language film Chala Murari Hero Banne by Asrani.&#91;197&#93;\n\nAap Ki Adalat, an Indian talk show which airs on India TV, featured an interview with Vajpayee just before the 1999 elections.&#91;198&#93; Pradhanmantri (lit.&#8201;'Prime Minister'), a 2013 Indian documentary television series which aired on ABP News and covers the various policies and political tenures of Indian PMs, includes the tenureship of Vajpayee in the episodes \"Atal Bihari Vajpayee's 13 days government and India during 1996–98\", \"Pokhran-II and Kargil War\", and \"2002 Gujarat Riots and Fall of Vajpayee Government\".&#91;199&#93;\n\nAbhishek Choudhary wrote an original portrait of Hindutva's first prime minister in VAJPAYEE: The Ascent of the Hindu Right, 1924–1977.&#91;200&#93; The book won the 2023 Tata Literature Live! First Book Award.&#91;201&#93;\n\nIn 2019, Shiva Sharma and Zeeshan Ahmad, owners of Amaash Films, acquired the official rights of the book The Untold Vajpayee written by Ullekh N P, to make a biopic based on Vajpayee's life from his childhood, college life and finally turning into a politician.&#91;202&#93;&#91;203&#93;&#91;204&#93;\n\nHindi-language film \"Main Atal Hoon\", starring Pankaj Tripathi as Vajpayee, was theatrically released in India on 19 January 2024.&#91;205&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Atal_Bihari_Vajpayee"
  },
  {
    "title": "Kishore Kumar",
    "content": "\n\n\nKishore Kumar (born Abhas Kumar Ganguly; pronunciationⓘ; 4 August 1929 – 13 October 1987) was an Indian playback singer, musician and actor.&#91;11&#93; He is widely regarded as one of the greatest, most influential and dynamic singers in the history of modern Indian music.&#91;12&#93;&#91;13&#93;&#91;14&#93;&#91;15&#93; Kumar was one of the most popular singers in the Indian subcontinent, notable for his yodelling and ability to sing songs in different voices.&#91;16&#93; He used to sing in different genres but some of his rare compositions, considered classics, were lost in time.&#91;17&#93;&#91;18&#93;&#91;19&#93; In 2013, Kumar was voted \"The Most Popular Male Playback Singer\" in a poll conducted by the Filmfare magazine.&#91;20&#93;   \n\nBesides Hindi, he sang in many other Indian languages, including Bengali, Marathi, Assamese, Gujarati, Kannada, Bhojpuri, Malayalam, Odia and Urdu.&#91;21&#93; He also released a few non-film albums in multiple languages, especially in Bengali, which are noted as all-time classics.&#91;22&#93; According to his brother and legendary actor Ashok Kumar, Kishore Kumar was successful as a singer because his \"voice hits the mike, straight, at its most sensitive point\".&#91;23&#93;\n\nHe won 8 Filmfare Award for Best Male Playback Singer out of 28 nominations and holds the record for winning and nominating the most Filmfare Awards in that category.&#91;22&#93; He was awarded the Lata Mangeshkar Award by the Madhya Pradesh government in 1985. In 1997, the Madhya Pradesh Government initiated an award called the \"Kishore Kumar Award\" for contributions to Hindi cinema. In 2012, his unreleased last song sold for ₹15.6 lakh ($185,000 USD) at the Osian's Cinefan Auction in New Delhi.&#91;24&#93;&#91;25&#93;&#91;26&#93;\n\nKishore Kumar was born in a Bengali Brahmin Ganguly&#91;27&#93;&#91;28&#93; family in Khandwa, Central Provinces (now in Madhya Pradesh) as Abhas Kumar Ganguly.&#91;29&#93; His father, Kunjalal Ganguly (Gangopadhyay) was a lawyer and his mother, Gouri Devi came from a wealthy Bengali family and was a home-maker. Kunjalal Gangopadhyaya was invited by the Kamavisadar Gokhale family of Khandwa to be their personal lawyer. Kumar was the youngest of four siblings, the older three being Ashok (the eldest), Sati Devi, and Anoop.&#91;30&#93;\n\nWhile Kumar was still a child, his brother Ashok became an actor in Hindi cinema. Later, Anoop also ventured into cinema with Ashok's help.&#91;31&#93; Kumar graduated from Christian College, Indore.&#91;32&#93;\n\nAfter Ashok Kumar became a star of Hindi films, the Ganguly family visited Bombay (now Mumbai) regularly. Abhas Kumar changed his name to 'Kishore Kumar' and started his cinema career as a chorus singer at Bombay Talkies, where his brother worked. Music director Khemchand Prakash gave Kumar a chance to sing \"Marne Ki Duayen Kyon Mangu\" for the film Ziddi (1948). After this, Kumar was offered many other assignments, but he was not very serious about a film career.&#91;33&#93; In 1949, he settled in Bombay.&#91;34&#93;\n\nDuring the initial stage of his career, Kumar was deeply inspired by singer K. L. Saigal and imitated his style of singing in some of his early films but later evolved his own, unique style.&#91;35&#93; He had a great respect for poet and musician Rabindranath Tagore who influenced him in many ways.&#91;36&#93;\n\nHe was an ardent admirer of Hollywood actor-singer Danny Kaye. He hung the portraits of all these three personalities at his Gouri Kunj residence and would bow respectfully before them every day as a rule.&#91;37&#93;\n\nIn his later career, Kumar was also heavily influenced by playback singer Ahmed Rushdi and his liking towards Rushdi was to the extent that former paid a tribute at Royal Albert Hall in London to the latter by singing some of his songs.&#91;38&#93;\n\nKumar employed yodelling in many of his songs including; Yeh dil na hota bechara, Zindagi ek safar hai suhana, and Chala jata hoon.  The style eventually became an essential feature of his singing and was inspired by Jimmie Rodgers and Tex Morton.&#91;39&#93;\n\nIn the movie Half Ticket, for one of the songs – \"Aake Seedhi Lagi Dil Pe\" – the music director Salil Chowdhury had a duet in mind and wanted Kumar and Lata Mangeshkar to sing the song. However, since Lata Mangeshkar was not in town and Salil Chowdhury had to record that song before she could return, Kumar solved the problem by singing both the male and female parts of the song himself. The duet is actually for Pran and Kumar on the screen dressed as a woman. It just turned out to be fine as he did admirably well singing both in male and female voices.&#91;40&#93;\n\nMusic director S. D. Burman is credited with spotting Kumar's talent for singing. During the making of Mashaal (1950), Burman visited Ashok's house, where he heard Kumar imitating K. L. Saigal. He complimented him and told him that he should develop a style of his own, instead of copying Saigal.&#91;41&#93; Kumar eventually developed his own style of singing, which featured yodelling, which he had heard on the records of Tex Morton and Jimmie Rodgers.&#91;42&#93;&#58;&#8202;60&#8202;S. D. Burman kept making Kishore sing for Dev Anand from the 50s to the early 70s. S. D. Burman provided him the training and encouraged Kumar a lot, especially in the late 50s and early 60s, resulting in Kumar developing into a great singer in the future years.&#91;43&#93;\n\nS. D. Burman recorded Kumar's voice for Dev Anand's Munimji (1954), Taxi Driver (1954), House No. 44 (1955), Funtoosh (1956), Nau Do Gyarah (1957), Paying Guest (1957), Guide (1965), Jewel Thief (1967), Prem Pujari (1970), and Tere Mere Sapne (1971). He also composed music for Kumar's home production Chalti Ka Naam Gaadi (1958). Some of their songs were \"Maana Janaab Ne Pukara Nahin\" from Paying Guest (1957), \"Hum Hain Rahi Pyar Ke\" from Nau Do Gyarah (1957), \"Ai Meri Topi Palat Ke Aa\" from Funtoosh (1956), and \"Ek Ladki Bheegi Bhaagi Si\" from Chalti Ka Naam Gaadi.&#91;44&#93; Asha Bhosle and Kishore performed duets composed by Burman including \"Chhod Do Aanchal\" from Paying Guest (1957), \"Ankhon Mein Kya Ji\" from Nau Do Gyarah (1957), \"Haal Kaisa Hai Janaab Ka\" and \"Paanch Rupaiya Baara Aana\" from Chalti Ka Naam Gaadi (1958) and \"Arre Yaar Meri Tum Bhi Ho Gajab\" from Teen Deviyan (1965).&#91;45&#93;\n\nAs a singer, Kumar's work with many music directors in this period includes \"Ye Raatein Ye Mausam\" and \"Hum Toh Mohabbat Karega\" from Dilli Ka Thug, \"Piya Piya Mora Jiya\" from Baap Re Baap, \"Hello Hello Ji\" from Bombay Ka Chor, \"Michael Hai To Cycle Hai\" from Bewaqoof, \"Ae Haseeno Nazneeno\" from Chacha Zindabad, \"Zaroorat Hai Zaroorat Hai\" from Manmauji (1961), \"Likha Hai Teri Ankhon Mein\" from Teen Deviaan, \"Suno Jaana Suno Jaana\", \"Pyaar Baatke Chalo\" and \"Kya Teri Zulfein Hai\" from Hum Sab Ustaad Hai, \"Khoobsurat Haseena\" from Mr. X in Bombay, \"Gaata Rahe Mera Dil\" from Guide (1965), \"Sultana Sultana\" from Shreeman Funtoosh, \"Machalti Hui\" from Ganga Ki Lahren, \"Mera Dil Meri Jaan\" and \"Pyar Ka Jaahan Hotel\" from Jaalsaaz and \"Yeh Dil Na Hota Bechara\" from Jewel Thief (1967).&#91;46&#93;\n\nMusic director C. Ramchandra also recognised Kumar's talent as a singer.&#91;47&#93; They collaborated on songs including \"Eena Meena Deeka\" from Aasha (1957). Kumar's work includes \"Nakhrewaali\" from New Delhi (1956) by Shankar Jaikishan, \"C.A.T. Cat Maane Billi\" and \"Hum To Mohabbat Karega\" from Dilli Ka Thug (1958) by Ravi, and \"Chhedo Na Meri Zulfein\" from Ganga Ki Lahren (1964) by Chitragupta.&#91;12&#93;\n\nIn 1968, Rahul Dev Burman worked with Kumar on the soundtrack of the film Padosan (1968), in which Kumar sang \"Mere Saamne Wali Khidki Mein\" and \"Kehna Hai.\"  Padosan was a comedy featuring Kishore as a dramatist-musician, Mehmood as a Carnatic music and dance teacher, and Sunil Dutt as a simpleton named Bhola. Kishore's character was inspired by his uncle, Dhananjay Bannerjee, a classical singer.&#91;33&#93; The highlight of the film was a musical, comical duel between Kumar, Sunil Dutt and Mehmood: \"Ek Chatur Nar Karke Singaar.&#91;48&#93;\n\nIn 1969, Shakti Samanta produced and directed Aradhana. Kumar sang three songs in the film; \"Mere Sapnon Ki Rani\", \"Kora Kagaj Tha Ye Man Mera\" and \"Roop Tera Mastana\". Shakti Samanta suggested that Kumar sing the other songs too. When the film was released, Kumar's three songs established him as a leading playback singer of Hindi cinema.&#91;49&#93; Kumar won his first Filmfare award for \"Roop Tera Mastana\".&#91;50&#93;&#58;&#8202;54&#8202;\n\nKumar's first film appearance was in Shikari (1946), in which his brother, Ashok played the lead role. Kumar played the lead in the Bombay Talkies film Andolan (1951), directed by Phani Majumdar. Although he got some acting assignments with the help of his brother, he was more interested in becoming a singer. But Ashok wanted Kumar to be an actor like him.&#91;41&#93; Between 1946 and 1955, Kumar appeared in 22 films, of which 16 were flops; since he was uninterested in taking up acting as a career, he found ways to be in the bad books of the director or producer, so that they would throw him from their films. It was only after the success of films such as Ladki, Naukari, Miss Mala, Char Paise and Baap Re Baap that Kumar developed an interest in acting seriously, which resulted in him having successful films as the lead actor between 1955 and 1966.&#91;15&#93;\n\nKumar starred in Bimal Roy's Naukari (1954) and Hrishikesh Mukherjee's directorial debut Musafir (1957). Salil Chowdhury, the music director for Naukari, was initially dismissive of Kumar as a singer when he found that Kumar had no formal training in music.&#91;47&#93; However, after hearing his voice, Chowdhury gave him the song Chhota Sa Ghar Hoga, which was supposed to be sung by Hemant Kumar.&#91;51&#93;\nThe commercially successful films of Kumar included Ladki (1953), Naukari (1954), Baap Re Baap (1955), Paisa Hi Paisa (1956), New Delhi (1956), Naya Andaz (1956), Bhagam Bhaag (1956), Bhai Bhai (1956), Aasha (1957), Chalti Ka Naam Gaadi (1958), Dilli Ka Thug (1958), Jaalsaaz (1959), Bombay Ka Chor (1962), Chacha Zindabad (1959), Man-Mauji (1962), Jhumroo (1961), Half Ticket (1962), Mr. X in Bombay (1964), Shreeman Funtoosh (1965), Ek Raaz (1963), Ganga Ki Lahren (1964), Hum Sab Ustaad Hai (1965), Haal E Dil, Pyar Kiye Jaa (1966), and Padosan (1968). As an actor, his best period was between 1954 and 1966. His onscreen pairing with actresses Mala Sinha, Vyjayanthimala, Nutan, Madhubala, Meena Kumari and Kumkum gave the biggest hits in his career.&#91;52&#93;\n\nChalti Ka Naam Gaadi (1958), his home production, had the three Ganguly brothers and Madhubala in main roles. Kumar played a car mechanic who has a romance with a city girl; (Madhubala) with a subplot involving the brothers.&#91;42&#93;&#58;&#8202;29&#8202; Kumar acted in and composed the music for Jhumroo (1961), and wrote the lyrics for the film's title song, \"Main Hoon Jhumroo\". Later, he produced and directed Door Gagan Ki Chhaon Mein (1964), where he starred with Bengali actress Supriya Devi.&#91;53&#93; He also wrote the script and composed music for the film, which is about the relationship between a father (Kumar) and his deaf and mute son (played by his real-life son Amit Kumar).&#91;50&#93;&#58;&#8202;52&#8202;\n\nAfter 1966, as an actor, Kumar built up a notoriety for coming late for the shootings or bunking them altogether.&#91;54&#93; His films flopped frequently after 1965 and he landed in income tax trouble.&#91;41&#93; Kumar produced and directed some movies in the late 1970s and early 1980s; Pyar Zindagi Hai, Badhti Ka Naam Dadhi (1978), Sabaash Daddy, Zindagi (1981), Door Wadiyon Mein Kahin (1980) and Chalti Ka Naam Zindagi (1982)—which was his last appearance as an actor.&#91;citation needed&#93;\n\nKumar was a leading singer throughout 1970s and 1980s until his death in 1987.&#91;55&#93;\nKumar sang the most songs in his career for Rajesh Khanna. Kumar sang 245 songs picturised on Rajesh Khanna across 92 films, which is an unbeaten record for singer-actor combination. Kishore sang 245 songs for Rajesh Khanna, 202 for Jeetendra, 131 for Amitabh Bachchan and 119 for Dev Anand.&#91;56&#93; S. D. Burman and Kishore continued with music including \"Ye dil na hota bechara\" and \"Aasmaan ke neeche\" from \"Jewel thief\" (1967), \"Phoolon Ke Rang Se\" and \"Shokhiyon Mein Ghola Jaaye\" from Prem Pujari (1969), \"Aaj Madhosh Hua Jaye Re,\" \"Khilte Hain Gul Yahan\" and \"O Meri Sharmilee\" from Sharmilee (1971), \"Meet Na Mila\" from Abhimaan (1973), and \"Jeevan Ki Bagiya Mehkegi\" from Tere Mere Sapne (1971). In 1975, S. D. Burman composed his last song for Kishore, \"Badi Sooni Sooni Hai\" for the film Mili.&#91;57&#93;\n\nR. D. Burman recorded several songs with Kumar in the 1970s, including \"O Maajhi Re\" from Khushboo, \"Yeh Shaam Mastaani\" and  \"Yeh Jo Mohabbat Hai\" from Kati Patang (1971), \"Raat Kali Ek Khwab Mein Aayi\" from Buddha Mil Gaya (1971) and \"Chingari Koi Bhadke\", \"Kuch To Log Kahenge (Amar Prem)\", \"Zindagi Ke Safar Me Guzar Jaate Hain Jo Makam\" from Aap Ki Kasam (1974), \"Aane Wala Pal\" from Golmaal (1979), \"Hume Aur Jeene Ki Chahat Na Hoti\" from Agar Tume Na Hote (1983), \"Raha Pe Rahete Hai\" from  Namkeen (1985) and \"Jab Bhi Koi Kangana\" from Shaukeen (1987). Although Kumar was not formally trained in classical music, R. D. Burman often had Kumar sing semi-classical songs, such as \"Humein Tum Se Pyaar Kitna\" from Kudrat and \"Mere Naina Saawan Bhadon\" from Mehbooba.&#91;19&#93;\n\nR. D. Burman recorded several duets pairing Kishore with Asha Bhosle and Lata Mangeshkar, including  \"Panna Ki Tamanna\" and \"Bahut Door Mujhe\" from Heera Panna (1973), \"Neend Chura Ke Raaton Mein\" from the film Shareef Budmaash (1973), \"Mujhko Mohabbat Mein Dhoka\" and \"Kisise Dosti Karlo\" from Dil Deewana (1974), \"Dhal Gayi Rang\" from Heeralal Pannalal (1978), \"Ek Main Hoon\" from Darling Darling (1977), \"Rimjhim Gire Sawan\" from Manzil (1979), \"Kya Yehi Pyar Hai\" and \"Hum Tum Se Mile\" from Rocky (1981), \"Jaan-e-Jaan Dhoondta\" from Jawani Diwani, \"Kahin Na Jaa\" and \"Kaho Kaise Rasta\" from Bade Dilwala (1983), \"Sun Zara Shok Haseena\" and \"Kharishoo\" from Harjaee (1981), \"Waada Haanji Waada\" from The Burning Train (1980) and \"Kaisi Lagrahi Hoon Mein\" from Jhuta Sach (1984).&#91;58&#93;\n\nApart from the Burmans, Kumar worked with other famous music directors too. The composer duo Laxmikant–Pyarelal (L-P) composed many songs sung by him, including \"Mere Mehboob Qayamat Hogi\" from Mr. X in Bombay, \"Mere Naseeb Mein Aye Dost\" from Do Raaste, \"Yeh Jeevan Hai\" from Piya Ka Ghar, \"Mere Dil Mein Aaj Kya Hai\" from Daag, \"Nahi Mai Nahi Dekh Sakta\" from Majboor, \"Mere Diwanepan Ki Bhi\" from Mehboob Ki Mehndi, \"Naach Meri Bulbul\" from Roti, \"Chal Chal Chal Mere Haathi\" from Haathi Mere Saathi and \"Tu Kitne Baras Ki\" from Karz.  L-P also worked with Kishore and Mohammed Rafi on duets for the films Zakhmee, Dostana, Ram Balram and Deedaar-E-Yaar. L-P composed \"I Love You (Kaate Nahin Katate Yeh Din Yeh Raat)\" for Mr. India (1987), a duet with Kishore and Alisha Chinoy.\n\nSalil Chowdhury recorded songs such as \"Koi Hota Jisko Apna\" from Mere Apne and \"Guzar Jaaye Din Din\" from Annadata. Ravindra Jain recorded \"Ghungroo Ki Tarah\" and the duets \"Le Jaayenge Le Jaayenge\" from Chor Machaye Shor and  \"Tota Maina Ki Kahani\" from Fakira. Shyamal Mitra recorded a duet of Kishore with Asha – Sara Pyaar Tumhara for the film Anandshram and solos \"Dil aisa kisi ne mera toda\" and \" Na poochho koi humein\" for the film \"Amanush\".\n\nKhayyam recorded many of Kishore's duets with Lata Mangeshkar, including \"Hazaar Raahein\" and \"Ankhon Mein Humne\" from Thodisi Bewafaii and \"Chandni Raat Mein\" from Dil-E-Nadaan (1982). Hridaynath Mangeshkar recorded \"Zindagi Aa Raha Hoon Main\" from Mashaal (1984).\n\nKalyanji-Anandji recorded several songs with Kishore including \"Zindagi Ka Safar\" and \"Jeevan Se Bhari\", from Safar (1970), \"O Saathi Re\" from Muqaddar Ka Sikandar (1978), \"Pal Pal Dil Ke Paas\" from Blackmail (1973), \"Neele Neele Ambar Par\" from Kalaakaar (1983) and the chart buster qawwali \"Qurbani Qurbani\" from Qurbani (1980).&#91;59&#93;\n\nKishore also worked with other composers including Rajesh Roshan, Sapan Chakraborty and Bappi Lahiri. Kumar sang \"Bhool Gaya Sab Kuchh\" (duet with Lata Mangeshkar) and \"Dil Kya Kare Jab Kisise\" for Rajesh Roshan's film Julie.&#91;60&#93; Their other songs include \"Yaadon Mein Woh\" from Swami, \"Chhookar Mere Man Ko Kiya Toone Kya Ishaara\" from Yaarana, \"Kaha Tak Ye Manko Andher Chalenge\" from Baton Baton Mein, \"O Yara Tu Yaro Se Hai Pyar\", and \"Laharon Ki Tatah Yaadien\" (1983) and Kahiye, Suniye (duet with Asha Bhosle) from Baton Baton Mein. Bappi Lahiri also recorded many songs with Kumar, including Pag Ghunghroo Bandh from Namak Halaal (1982), Manzilen Apni Jagah Hai from Sharaabi (1984), \"Chalate Chalte Mere Ye Geet Yad Rakhana\" from Chalte Chalte (1975) and \"Saason Se Nahi Kadmose Nahi\" from Mohabbat (1987) and duets with Lata Mangeshkar such as \"Albela Mausam\" and \"Pyar Ka Tohfa\" from Tohfa (1984). The Kishore and Bappi pair also recorded hits in Bengali, including \"Chirodini Tumi Je Amar\" from Amar Sangi (1987) and \"E Amar Gurudakshina\" from Gurudakshina (1987). Another Bengali musician was Ajay Das, who composed many hit songs in Kumar's voice. He also recorded a duet song Hello Hello Kya Haal Hai with Asha Bhosle for Naushad in 1975 for the movie Sunehra Sansar, the only song of Kishore. He also worked with music directors Basu and Manohari Singh for duets such as \"Wada Karo Jaanam\" and \"Dariya Kinare\" for the film Sabse Bada Rupaiya and \"Aa Humsafar\" for the film Chatpatee.&#91;citation needed&#93;\n\nDuring the Indian Emergency (1975–1977), Sanjay Gandhi asked Kishore to sing for an Indian National Congress rally in Bombay, but he refused.&#91;61&#93; As a result, Information and broadcasting minister Vidya Charan Shukla (1975–1977) put an unofficial ban on playing Kumar songs on state broadcasters All India Radio and Doordarshan from 4 May 1976 till the end of Emergency.&#91;62&#93;&#91;63&#93;\n\nKishore Kumar's son Amit Kumar became a singer in Hindi cinema in 1974 with the song \"Apne Bas Ki Baat Nahi\", composed by Kumar for the film Badthi Ka Naam Daadi. Amit Kumar became popular with success of the song \"Bade Achche Lagte Hai\". Kishore continued singing for several actors even in 1980s. Kumar performed stage shows right from 1969 to earn money to pay his income tax arrears.&#91;54&#93; Kumar stopped singing for Amitabh Bachchan in the year 1981, after Bachchan refused to appear as a guest in the film Mamta Ki Chhaon Mein, which Kishore produced. Kishore declined to give voice for Amitabh in Naseeb, Coolie, Mard. Kishore said he would give his voice to Randhir Kapoor in the film Pukar, also starring Amitabh. Since Kishore shared good rapport with R. D. Burman, he agreed to sing in Mahaan, Shakti and Bemisal. He also agreed to Amitabh for the superhit films Sharabi and Geraftaar, at the request of Bappi Lahiri. Later, Kishore called a truce by singing for Amitabh in a solo song in Shahenshah and later in Toofan.&#91;64&#93; Kishore sang the song \"Mera Geet Adhura Hai\" for his production Mamta Ki Chaon Mein and picturised the song on Rajesh Khanna. Kishore had directed the film, but died in 1987 and Rajesh Khanna helped Amit Kumar in releasing the film in 1989. He also temporarily stopped singing for Mithun Chakraborty after Yogeeta Bali divorced him and married Chakraborty.&#91;65&#93; However, he later sang for Chakraborty in Surakshaa (1979), and in the 1980s in many films, including Boxer, Jaagir, Faraib and Waqt Ki Awaz.\n\nIn the mid-1980s, Kishore sang for Anil Kapoor in his first Hindi film as a leading man, Woh Saat Din. He also recorded for Mashaal. (1984), Saaheb. (1985), Karma. (1986), Janbaaz.(1986) and Mr. India. (1987) for which he recorded the song \"Zindagi Ki Yahi Reet Hai Haar Ke Baad Hi Jeet Hai\". He sang duets with Alka Yagnik such as \"Tumse Badhkar Duniya Mein Na Dekha\" for Kaamchor in 1982, \"Humnashi Aaake from Ek Daku Saher Mein\" and sang \"Teri Meri Prem Kahani\" in Pighalta Aasman. He also recorded for newcomers like Aditya Pancholi for the song \"Mere Dil Mein Utar Jana\" for the film Laal Paree (1991).\n\nKishore Kumar was highly popular in 80s, winning five Filmfare Award for Best Male Playback Singer for the films Thodisi Bewafaii (1980), Namak Halaal (1982), Agar Tum Na Hote (1983), Sharaabi (1984), Saagar (1985). Kishore Kumar was the single nominee in 1985, having all the four nominations to his credit. Incidentally, Kishore Kumar's all nominated songs were from a single movie, Sharaabi.\n\nHe had recorded the duets \"Kaho Kahan Chale\" for the film Bulundi, \"Pyar Ka Dard Hai\" from Dard and \"Tum Jo Chale Gaye\" from Aas Paas, a few days before his heart attack in 1981. He suffered his first heart attack on 24 January 1981 in Kolkata in the noon hours and within a gap of another four hours, suffered his second heart attack. The first solo song sung by him, after recovery from his two attacks was \"Mere Sang Sang Aya\" from Rajput (1982) and the duet with Asha – \"Mausam Bheega Bheega\" from Gehra Zakham.\n\nKishore debut singing in Marathi films in 1987 with the song \"Ashwini Ye Na\" from the movie Gammat Jammat. This song became extremely popular and is considered one of the best songs in Marathi cinema. Kishore also sang another Marathi song, \"Tujhi Majhi Jodi Jamali\". Throughout his career, Kishore sang only two songs in Marathi.&#91;66&#93;\n\nKishore Kumar is credited with the highest number of multi-singer and male duet hits with the finest singers of different eras. Kishore Kumar sang the highest number of duets with Asha Bhosle (687 duets). He also sang duets with Lata Mangeshkar, Mohammed Rafi, Mukesh, Manna Dey, Mahendra Kapoor, Geeta Dutt, Shamshad Begum, Sulakshana Pandit, S. Janaki, P. Susheela and Kavita Krishnamurthy.\n\nIn the song \"Humko Tumse Ho Gaya Hai Pyar\" from Amar Akbar Anthony, Kishore sang one song with Lata Mangeshkar, Mohammed Rafi and Mukesh, the most legendary singers in Hindi films. This was probably the only time that all of them rendered their voices for one song.&#91;67&#93; Kishore sang over 30 songs with his contemporary Rafi and they were good friends despite the claims of animosity reported in the media.&#91;68&#93;\n\nKishore Kumar sang Bhajans like \"Aao Kanhai Mere Dham\" from Mere Jeevan Saathi (1972), \"Devi Mata Rani\" from Swarag Se Sunder (1986), \"Jai Bholenath Jai Ho Prabhu\" from Kunwara Baap (1974), \"He Re Kanhaiya\" from Chhoti Bahu (1971), \"Jab Ram Naam Le Le\" from Abhi Toh Jee Le (1977), \"Kahe Apno K Kaam Nahi Aye Tu\" from Raampur Ka Lakshman (1972), \"Krishna Krishna, Bolo Krishna\" from Naya Din Nai Raat (1974), \"Prabhuji Teri Leela Aparampaar\" from Humsafar (1953) etc.&#91;69&#93;&#91;70&#93;\n\nKishore Kumar recorded Qawwalis like all-time hit \"Vaada Tera Vaada\" from Dushman (1971), \"Hum Toh Jhuk Kar Salam Karte Hai\" from Fakira (1976), \"Mehfil Mein Paimana Jo Laga Jhumne\" from Chunaoti (1980), \"Is Ishq Mein\" from Mr. Romeo (1974), \"Kya Cheez Hai Aurat Duniya Mein\" from Zorro (1975), \"Haal Kya Hai Dilon Ka\" from Anokhi Ada (1973), a semi qawwali \"Jab Se Sarkar Ne Nashabandi Tod Di\" from 5 Rifles (1974), the chart buster qawwali \"Qurbani Qurbani Qurbani\" from Qurbani (1980) etc.&#91;59&#93;&#91;71&#93;\n\nKishore Kumar sang Ghazals like \"Peechhli Yaad Bhula Do\" from Mehndi (1983), \"Aisi Haseen Chandni\" from Dard (1981), \"Yun Neend Se Woh Jaan-E-Chaman\" from Dard Ka Rishta (1982), \"Tera Chehra Mejhe Gulab Lage\" from Aapas Ki Baat (1981) and \"Sarakti jaaye hai rukh se naqaab\" from film Deedar e Yaar (1982) with Lata Mangeshkar under the music direction of Laxmikant Pyarelal and penned by famous Urdu shayar Ameer Meenai and many more.&#91;71&#93;&#91;72&#93;\n\nKumar married four times. His first wife was Bengali singer and actress Ruma Guha Thakurta aka Ruma Ghosh. Their marriage lasted from 1950 to 1958.&#91;50&#93;&#58;&#8202;53&#8202;\n\nKumar's relationship with actress Madhubala was the subject of much media attention. Madhubala had worked with Kumar in many films including his home production Chalti Ka Naam Gaadi (1958) and Jhumroo (1961).&#91;citation needed&#93; According to Madhubala's sister Madhur Bhushan, after Madhubala's relationship with actor Dilip Kumar faded, Madhubala got involved with Kishore who was going through a divorce with Ruma Guha Thakurta. Their love affair went on for three years through Chalti Ka Naam Gaadi and Half Ticket.&#91;73&#93; Madhubala was ill and was planning to go to London for treatment. She had a ventricular septal defect (hole in the heart). The couple had a civil wedding in 1960. Kumar reportedly converted to Islam and changed his name to Karim Abdul,&#91;74&#93; but Madhubala's sister has refuted such claims, saying that Kumar remained a Hindu.&#91;75&#93; His parents refused to attend the ceremony. The couple also had a Hindu wedding ceremony to please Kumar's parents. Their marriage ended with Madhubala's death on 23 February 1969.&#91;76&#93;\n\n\"I knew she (Madhubala) was very sick even before I married her. But a promise is a promise. So I kept my word and brought her home as my wife, even though I knew she was dying from a congenital heart problem. For 9 long years, I nursed her. I watched her die before my own eyes. You can never understand what this means until you live through this yourself. She was such a beautiful woman and she died so painfully. She would rave and rant and scream in frustration. How can such an active person spend 9 long years bed-ridden? And I had to humour her all the time. That's what the doctor asked me to. That's what I did till her very last breath. I would laugh with her. I would cry with her.\"&#91;77&#93;\n\nKumar's third marriage was to Yogeeta Bali, and lasted from 1976 to 4 August 1978. Kumar was married to Leena Chandavarkar from 1980 until his death. He had two sons, Amit Kumar with Ruma, and Sumeet Kumar with Leena Chandavarkar.&#91;78&#93;\nKumar is said to have been paranoid about not being paid.&#91;33&#93; During recordings, he would sing only after his secretary confirmed that the producer had made the payment.&#91;79&#93; On one occasion, when he discovered that his dues had not been fully paid, he appeared on set with makeup on only one side of his face. When the director questioned him, he replied \"Aadha paisa to aadha make-up.\" (Half make-up for half payment).&#91;33&#93; On the sets of Bhai Bhai, Kumar refused to act because the director M. V. Raman owed him ₹ 5,000. Ashok Kumar persuaded him to do the scene but when the shooting started, Kishore walked a few paces and said, Paanch Hazaar Rupaiya (five thousand rupees) and did a somersault. After he reached the end of the floor, he left the studio.&#91;80&#93; On another occasion, when producer R.C. Talwar did not pay his dues in spite of repeated reminders, Kumar arrived at Talwar's residence shouting \"Hey Talwar, de de mere aath hazaar\" (\"Hey Talwar, give me my eight thousand\") every morning until Talwar paid up.&#91;79&#93;\n\nIn spite of his \"no money, no work\" principle, sometimes Kumar recorded free even when the producers were willing to pay. Such films include those produced by Rajesh Khanna and Danny Denzongpa.&#91;81&#93; On one occasion, Kishore helped actor-turned-producer Bipin Gupta by giving him ₹ 20,000 for the film Daal Me Kala (1964). When actor Arun Kumar Mukherjee—one of the first persons to appreciate Kishore's singing talent—died, Kumar regularly sent money to Mukherjee's family in Bhagalpur.&#91;80&#93;\n\nAccording to Kumar's eldest son Amit Kumar, Kumar did a lot of charity and free shows for jawans and cancer patients but he never spoke about it. Kumar was a homebody and avoided the trappings of stardom. He was an early riser and went to bed early. He loved authentic Bengali food, and was deeply emotional, a side which he seldom expressed and was also philosophic. Kumar was fond of Biblical films and liked sitting in a cemetery.&#91;82&#93;\n\nThe film Anand (1971) was originally supposed to star Kishore and Mehmood Ali in the lead.&#91;83&#93; Hrishikesh Mukherjee, the director of the film, was asked to meet Kishore to discuss the project. However, when he went to Kumar's house he was driven away by the gatekeeper due to a misunderstanding. Kumar—himself a Bengali—had not been paid for a stage show organised by another Bengali man and had instructed his gatekeeper to drive away this \"Bengali\", if he ever visited the house. Consequently, Mehmood had to leave the film as well, and new actors (Rajesh Khanna and Amitabh Bachchan) were signed up for the film.&#91;83&#93;\n\nMany journalists and writers have written about Kumar's seemingly eccentric behaviour.&#91;84&#93;&#91;85&#93; He placed a sign that said \"Beware of Kishore\" at the door of his Warden Road flat. Once, producer-director H. S. Rawail, who owed him some money, visited his flat to pay the dues. Kumar took the money and when Rawail offered to shake hands with him, reportedly Kishore put Rawail's hand in his mouth, bit it and asked \"Didn't you see the sign?\". Rawail laughed off the incident and left quickly.&#91;80&#93; According to another reported incident, once Kumar was due to record a song for producer-director G. P. Sippy. As Sippy approached his bungalow, he saw Kumar going out in his car. Sippy asked Kumar to stop his car but Kumar increased his speed. Sippy chased him to Madh Island where Kumar finally stopped his car near the ruined Madh Fort. When Sippy questioned his strange behaviour, Kumar refused to recognise or talk to him and threatened to call the police. The next morning, Kumar reported for the recording session. An angry Sippy questioned him about his behaviour the previous day but Kumar said that Sippy must have dreamt the incident and said that he was in Khandwa on the previous day.&#91;86&#93;\n\nOnce, a producer went to court to get a decree that Kumar must follow the director's orders. As a consequence, he obeyed the director to the letter. He refused to alight from his car until the director ordered him to do so. After filming a car scene in Bombay, Kumar drove until he reached Khandala because the director forgot to say \"Cut\".&#91;80&#93; In the 1960s, a financier named Kalidas Batvabbal, who was disgusted with Kumar's alleged lack of cooperation during the shooting of Half Ticket, reported to the income tax authorities, who raided his house. Later, Kumar invited Batvabbal to his home, asked him to enter a cupboard for a chat and locked him inside. He unlocked Batvabbal after two hours and told him, \"Don't ever come to my house again\".&#91;80&#93;\n\nKumar was a loner; in a 1985 interview with Pritish Nandy he said that he had no friends—he preferred talking to his trees instead.&#91;87&#93; Once, when a reporter made a comment about how lonely he must be, Kumar took her to his garden, named some of the trees there and introduced them to the reporter as his closest friends.&#91;80&#93;\n\nBy September 1987, Kishore Kumar had decided to retire as he was unhappy with kind of songs and tunes being made by music directors and was planning to return to his birthplace Khandwa.&#91;54&#93;\n\nOn 13 October 1987, on his brother Ashok Kumar's 76th birthday, he died of a heart attack in Bombay at 4:45 pm. His body was taken to Khandwa for cremation. Kishore had recorded his last song, \"Guru Guru\", a duet with Asha Bhosle for the film Waqt Ki Awaz (1988) composed by Bappi Lahiri for Mithun Chakraborty and Sridevi, the day before he died.&#91;88&#93;&#91;89&#93;\n\nKumar's funeral procession was one of the largest for a non-political film personality, with an estimated crowd of around 5,000-7,000 people in various en-routes of Bombay including his house in Gauri Kunj, Bandra, Santa Cruz, Linking Road, Khar, Dharavi and R.K. Studios in Chembur, and an even bigger crowd estimated between 90,000 to 1,50,000 when his body had reached his hometown in Khandwa for cremation as per his last wish. [1]&#91;90&#93;&#91;91&#93;.The entire film industry, had plunged into grief and shock due to his untimely passing away with an outpouring of tributes from various veterans from the industry including Raj Kapoor, Dev Anand,  Rajesh Khanna,  Pran, Nutan,  Manoj Kumar, Vinod Khanna, Rajendra Kumar, Randhir Kapoor, Om Prakash, Vinod Mehra, B.R. Chopra, Suresh Oberoi, Danny Denzongpa, Lata Mangeshkar,  Asha Bhosle, Hemlata, Mohammed Aziz, Shabbir Kumar,  Bappi Lahiri, R.D. Burman, Kalyanji-Anandji, Ravindra Jain, Sulakshana Pandit, Lalit Pandit, Vijayta Pandit, Shyamal Mitra, Anand Bakshi,  Gulzar,  Khayyam, Balasaheb Thackeray, Salil Chaudhary, Tanuja Mukherjee, Saira Banu, Pradeep Kumar,Poonam Dhillon,  Deepti Naval, Mala Sinha, Nadeem-Shravan, Abhijeet Bhattacharya, Alka Yagnik, Vijay Anand, Chetan Anand, Johnny Lever, Amrish Puri, Anu Malik, Javed Akhtar, Salim Khan, Uttam Singh, and many others. [2]\n\nRecalling the day of Kishore Kumar's death and his last lines before he took his last breath, his wife Leena Chandavarkar was quoted as saying:\n\"On the morning of October 13 (the day Kishore Kumar passed away in 1987), he looked pale and as though in deep sleep. As I went near him, he woke up and asked, 'Did you get scared? Today is my holiday'.\" That day he had several meetings at home. During lunch he told me that we'd watch the film River of No Return in the evening. A little later, I heard him move furniture in the next room. When I went to see what was happening, I saw him lying on the bed. Nervously he said, 'I'm feeling weak'. I ran to call the doctor. He got angry and said, 'If you call the doctor, I'll get a heart-attack'. Those were his last lines. His eyes were wide open and he was breathing out. I thought he was fooling as usual but that was the end.\"&#91;citation needed&#93;\n\nSingers like Kumar Sanu, Abhijeet Bhattacharya,  Ayushmann Khurrana, Amit Kumar, Sudesh Bhosale, Vinod Rathod,  Roop Kumar Rathod&#91;92&#93; were influenced by Kishore Kumar's style of singing.&#91;93&#93;&#91;94&#93;&#91;95&#93;&#91;96&#93;&#91;97&#93;   Many other singers like KK, Mohit Chauhan, Shaan, Neeti Mohan, Shilpa Rao have been inspired by Kumar.&#91;98&#93;&#91;99&#93;&#91;100&#93;&#91;101&#93;&#91;102&#93;\n\nKingdom of Dreams, a project of the Great Indian Nautanki Company jointly owned by Wizcraft International Entertainment and Apra Group launched a musical comedy Jhumroo on 7 April 2012 which is a tribute to Kumar and his legacy to music and Hindi films. The musical comedy features 19 retro songs of Kumar's.&#91;103&#93;&#91;104&#93;\n\nKumar was inducted into the Bollywood Walk of Fame at Bandra Bandstand, where his autograph was preserved.&#91;105&#93;\n\nA statue of Kumar was installed on Southern Avenue, Kolkata on 22 October 2018.&#91;106&#93;&#91;107&#93;\n\nAn album titled Baba Mere made by Kumar's son Amit was released on his 86th birthday. It shows Kumar's granddaughter Muktika Ganguly waking up in the middle of the night and in her dreams she meets her grandfather. The album is on a big canvas and is like a film. Amit Kumar termed the album as the greatest tribute to his father Kishore Kumar.&#91;108&#93;&#91;109&#93;\n\nIn 2015, the UK-based newspaper Eastern Eye placed Kumar fourth in their \"Greatest 20 Bollywood Playback Singers\" list.&#91;110&#93;  Kumar was also included among the top ten most searched Indian singers on Google Search in 2016.&#91;111&#93;\n\nIn the Outlook Music Poll conducted by Outlook Magazine in June 2010, three songs were tied for the No. 2 place: One was sung by Kishore. The song was \"Kuch to log kahenge logon ka kaam hai kehna\" (Amar Prem, 1972). This poll was published in Outlook. The jury included people in the Indian music industry.&#91;citation needed&#93;  \n\nIn 2013, Kumar was voted \"The Most Popular Male Playback Singer\" in a poll conducted by the Filmfare magazine. Kumar won the poll with 38% of the votes.&#91;112&#93;   \n\nMarking the centenary of Hindi Cinema, CNN-IBN conducted a poll about the \"Greatest Voice of Hindi Cinema\" in which Kishore Kumar won the second place with 33.6% of the votes. The poll saw neck-to-neck competition between Rafi (who ultimately attained victory by a slim margin of 1.72% having gained 35.32%) and Kishore Kumar, followed by Lata Mangeshkar (24.59%), Mukesh (3.33%) and Asha Bhosle (3.16%) respectively. [3]\n\nThere have been appeals to the Government of India to honour the singer-actor posthumously with Bharat Ratna(India's Highest Civilian Award).&#91;113&#93;&#91;114&#93;\n\nManna Dey his contemporary once said that \"Kishore had the best voice of all playback singers. He made singing sound so effortless. He had no classical training but could surpass me in a song because of his wonderful musical instinct and natural singing flair.\"&#91;115&#93;\n\nHindi films Playback Singer Asha Bhosle has said that:&#173;\n\nActor Rajesh Khanna said \"Kishore Kumar was my soul and I was his body.\"&#91;117&#93;\n\nActress Vyjayanthimala said:\n\nHindi film superstar Amitabh Bachchan called Kishore Kumar a multitalented genius who shall remain a phenomenal star.&#91;119&#93;\n\nFormer cricketer and batsman Sachin Tendulkar said:\n\nKumar's unreleased song was sold for Rs 15.6 lakh at the Osian's Cinefan Auction, New Delhi in 2012, the highest price bid for any Indian singer. The song was \"Tum hi to woh ho\", written by Kulwant Jani with music by Usha Khanna. This was for a film called \"Khel Tamasha\" by Rakesh Kumar, which never got made. The song was recorded just three days before his death in October 1987.&#91;26&#93;\n\nToday Kumar's popularity spans across many countries of South Asia, especially Pakistan.&#91;121&#93;&#91;122&#93;&#91;123&#93;\n\nDigitally-colorized version of Kumar's film—Half Ticket (in 2012)—has been released theatrically.&#91;124&#93;\n\nSony TV organised the television singing contest K For Kishore to search for a singer like Kumar.&#91;citation needed&#93;\n\nToday Kumar's popular songs continue to be remixed or recreated.&#91;125&#93;&#91;126&#93;\n\nIn his memory, the government of Madhya Pradesh has set up a memorial on the outskirts of Khandwa. It is open to public and has his life sized statue in a lotus-shaped structure. It also houses a mini-theatre and museum dedicated to him. On his birth and death anniversary each year, a function is held and many fans participate. The mini-theatre also screens his films on these days.&#91;127&#93;&#91;128&#93;&#91;129&#93;\n\nA large number of musical tributes, special programmes and functions are inspired on his birth and death anniversaries every year. As well as fans of Kumar and music lovers from Khandwa make offerings of 'doodh jalebi', which was Kumar's favourite dish at his memorial annually on his birth anniversary.&#91;130&#93;&#91;131&#93;&#91;132&#93;\n",
    "url": "https://en.wikipedia.org/wiki/Kishore_Kumar"
  }
]
